{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dateutil import parser as dparser\n",
    "import urllib.parse\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from util.config import config\n",
    "from util.pyRanker import BM25\n",
    "from util.web_query import web_query\n",
    "from util.ticker import Ticker\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "classifier = pipeline(task='sentiment-analysis',model=model,tokenizer=tokenizer)\n",
    "max_tokens = int(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running through sub_docs and outputting sentiment list\n",
    "\n",
    "def get_sentiments(docs):\n",
    "    \n",
    "    c = 0\n",
    "    for x in docs.keys():\n",
    "        c+=len(docs[x])\n",
    "    \n",
    "    pgres = widgets.IntProgress(value=0,min=0,max=c, step=1)\n",
    "    display(pgres)\n",
    "    \n",
    "    sentiments = {}\n",
    "    \n",
    "    for x in docs.keys():\n",
    "        scrs=[]\n",
    "        for y in range(0, len(docs[x])):\n",
    "            \n",
    "            s = classifier(docs[x][y])\n",
    "            scr = s[0]['score']\n",
    "            if s[0]['label']==\"NEGATIVE\":\n",
    "                scr=scr*-1\n",
    "            scrs.append(scr)\n",
    "            pgres.value+=1\n",
    "            pgres.description=str(pgres.value)+\":\"+str(c)\n",
    "        \n",
    "        sentiments[x]=scrs\n",
    "                \n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the rows of a 2d input_matrix so they sum to 1\n",
    "    \"\"\"\n",
    "\n",
    "    row_sums = input_matrix.sum(axis=1)\n",
    "    try:\n",
    "        assert (np.count_nonzero(row_sums)==np.shape(row_sums)[0]) # no row should sum to zero\n",
    "    except Exception:\n",
    "        raise Exception(\"Error while normalizing. Row(s) sum to zero\")\n",
    "    new_matrix = input_matrix / row_sums[:, np.newaxis]\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    #******************************************************************************\n",
    "    #----------------------------------Method---------------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    #Run the web_query to produce a collection of text documents scraped from the web\n",
    "    \n",
    "    #Use the set_results() function to store the full results in the corpus for processing\n",
    "    \n",
    "    #Use the set_corpus() function to assign the documents scraped from the web to the corpus\n",
    "    \n",
    "    #Sub divide the documents into smaller sub_docs\n",
    "    \n",
    "    #Rank the documents based on relevance to the original query as well as any tags\n",
    "    \n",
    "    #Prune the sub_docs to produce a relevant set\n",
    "    \n",
    "    #\n",
    "    \n",
    "    #******************************************************************************\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #typical corpus data\n",
    "        self.documents = []\n",
    "        self.vocabulary = []\n",
    "        self.number_of_documents = 0\n",
    "        self.vocabulary_size = 0\n",
    "                \n",
    "        #for web results\n",
    "        self.query_results=None\n",
    "        self.max_tokens=512\n",
    "        self.failed = []\n",
    "        \n",
    "        #sub dividing documents\n",
    "        self.tokenizer=None\n",
    "        self.sub_docs=None\n",
    "        self.sub_list=[]\n",
    "        \n",
    "        #queries\n",
    "        self.prime_q=None\n",
    "        self.expanded_q=None        \n",
    "        \n",
    "        #relevance scores\n",
    "        self.document_scores=None\n",
    "        self.subdoc_scores=None\n",
    "        self.title_scores=None\n",
    "        self.sub_list_scores=None\n",
    "                \n",
    "        #pruned data\n",
    "        self.pruned_docs=[]\n",
    "        self.pruned_subdocs={}\n",
    "        \n",
    "        #relevant\n",
    "        self.relevant_set={}\n",
    "        self.relevant_scores={}\n",
    "        self.rel_list_scores=[]\n",
    "        \n",
    "        #sentiments\n",
    "        self.sentiments=None\n",
    "    \n",
    "    \n",
    "    #******************************************************************************\n",
    "    #------------------------------Setting Corpus----------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def set_results(self, df):\n",
    "        #dataframe returned from webquery\n",
    "        self.query_results=df\n",
    "    \n",
    "    def set_corpus(self, documents):\n",
    "        self.documents = documents\n",
    "                \n",
    "\n",
    "    def build_corpus_from_file(self, file_path):\n",
    "\n",
    "        f = open(file_path, 'r')\n",
    "        docs = f.readlines()\n",
    "        for d in docs:\n",
    "            self.documents.append(d)\n",
    "        self.number_of_documents = len(docs)\n",
    "\n",
    "        \n",
    "    def build_vocabulary(self, stopwords):\n",
    "\n",
    "        v = set([])\n",
    "        for x in self.documents:\n",
    "            tmp = set(x.split())\n",
    "            tmp = {x for x in tmp if x.lower() not in stopwords}\n",
    "                        \n",
    "            v.update(tmp)\n",
    "        \n",
    "        v = list(v)\n",
    "        self.vocabulary = v\n",
    "        self.vocabulary_size = len(v)\n",
    "    \n",
    "    def build_queries(self, ticker, prime_w=1, tag_w=0.05, include_gtags=True, gtags_w=0.01, include_btags=True, btags_w=0.01):\n",
    "        name = re.sub('(,|\\.|Inc|inc|company|co )',\"\",str(ticker.name))\n",
    "        self.prime_q = name\n",
    "        \n",
    "        gtags = ['investing','analysis','analyst','upgrade','downgrade']\n",
    "        btags = ['sentiment','opinion','outlook']\n",
    "        \n",
    "        exp = []\n",
    "        for t in ticker.tags:\n",
    "            exp.append([t, tag_w])\n",
    "        if include_gtags:\n",
    "            for t in gtags:\n",
    "                exp.append([t, gtags_w])\n",
    "        if include_btags:\n",
    "            for t in btags:\n",
    "                exp.append([t, btags_w])\n",
    "        \n",
    "        self.expanded_q=exp\n",
    "    \n",
    "    #******************************************************************************\n",
    "    #------------------------------Sub Dividing-------------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def get_pgraphs(self, doc, cutoff, method):\n",
    "        #updated get_pgraphs() with method for cutoff\n",
    "        #cut off method:\n",
    "        #sen: number of sentences\n",
    "        #word: number of words  \n",
    "\n",
    "        pgraphs=[]\n",
    "        freshsoup = re.split('\\n\\n',doc)\n",
    "        for x in range(0,len(freshsoup)):\n",
    "            if method=='word':\n",
    "                words = len(str(freshsoup[x]).strip().split(' ',maxsplit=cutoff))\n",
    "                if words>cutoff:\n",
    "                    pgraphs.append(freshsoup[x])\n",
    "                    \n",
    "            elif method=='sen':\n",
    "                sens = len(re.findall(\"\\.\",str(freshsoup[x]).strip()))\n",
    "                if sens>cutoff:\n",
    "                    pgraphs.append(freshsoup[x])\n",
    "                    \n",
    "        return pgraphs\n",
    "    \n",
    "    def split_doc(self, doc):         \n",
    "        \n",
    "        if len(doc)>0:\n",
    "            avgwrds = 15\n",
    "            estsens = len(doc)/avgwrds\n",
    "        \n",
    "            if len(re.findall('\\.',doc))>=estsens:\n",
    "                cut_point = doc.rfind('.', 0, int(len(doc)/2))\n",
    "                if cut_point<=0:\n",
    "                    cut_point = int(len(doc)/2)\n",
    "            else:\n",
    "                cut_point = int(len(doc)/2)\n",
    "\n",
    "            d1 = doc[0:cut_point]\n",
    "            d2 = doc[cut_point+1:]\n",
    "            \n",
    "            tkns1 = int(len(self.tokenizer(d1)['input_ids']))\n",
    "\n",
    "            if tkns1>self.max_tokens:\n",
    "                self.split_doc(d1)\n",
    "            else:\n",
    "                if len(d1)>0:\n",
    "                    self.subs.append(d1)\n",
    "\n",
    "            tkns2 = int(len(self.tokenizer(d2)['input_ids']))\n",
    "\n",
    "            if tkns2>self.max_tokens:\n",
    "                self.split_doc(d2)\n",
    "            else:\n",
    "                if len(d2)>0:\n",
    "                    self.subs.append(d2)\n",
    "\n",
    "    \n",
    "    def get_subdocs(self, pgraphs):\n",
    "        #Updated get_subdocs with iterative slicing \n",
    "        #Changed to recursive slicing\n",
    "        #ensure sub_docs tokens will not exceed max_tokens for sentiment model\n",
    "        self.subs=[]\n",
    "\n",
    "        for x in range(0, len(pgraphs)):\n",
    "            sen_cnt = len(re.split('\\n|\\. ',pgraphs[x]))\n",
    "            tkns = int(len(tokenizer(pgraphs[x])['input_ids']))\n",
    "\n",
    "            if tkns<self.max_tokens:\n",
    "                self.subs.append(pgraphs[x])\n",
    "            else:\n",
    "                self.split_doc(pgraphs[x])\n",
    "        \n",
    "        return self.subs\n",
    "        \n",
    "    def sub_divide(self, tokenizer, cutoff=1, method='sen'):\n",
    "\n",
    "        #creates a dictionary of sub_docs divided from each document in the corpus\n",
    "        #method: using get_pgraphs() followed by get_subdocs()\n",
    "        #output form: dict{ document_id : [subdoc_1, subdoc_2 ... subdoc_n] }\n",
    "\n",
    "        subbed_data = {}\n",
    "        self.tokenizer=tokenizer\n",
    "        \n",
    "        if len(self.pruned_docs)==0:\n",
    "            self.prune_docs()\n",
    "        \n",
    "        for x in range(0, len(self.documents)):\n",
    "            \n",
    "            #only include documents that made the first relevance cut\n",
    "            if x in self.pruned_docs:\n",
    "                pg = self.get_pgraphs(self.documents[x], cutoff, method)\n",
    "                subs = self.get_subdocs(pg)\n",
    "                subbed_data[x]=subs\n",
    "        \n",
    "        self.sub_docs = subbed_data\n",
    "        self.sub_list=[]\n",
    "        for x in self.sub_docs.keys():\n",
    "            for y in self.sub_docs[x]:\n",
    "                self.sub_list.append(y)\n",
    "        \n",
    "\n",
    "        \n",
    "    #******************************************************************************\n",
    "    #----------------------------------Relevance Scoring---------------------------\n",
    "    #******************************************************************************  \n",
    "    \n",
    "    def rank_docs(self, ranker):\n",
    "        query = self.prime_q\n",
    "        self.document_scores = ranker.score(query, self.documents)\n",
    "        \n",
    "        \n",
    "    def rank_subdocs(self, ranker, expanded=False):\n",
    "        sub_vecs={}\n",
    "        \n",
    "        if expanded==False:\n",
    "            query=self.prime_q\n",
    "            for x in self.sub_docs.keys():\n",
    "                sub_vec = ranker.score(query, self.sub_docs[x])\n",
    "                sub_vecs[x]=sub_vec\n",
    "        else:\n",
    "            query = self.expanded_q\n",
    "            for x in self.sub_docs.keys():\n",
    "                sub_vec = ranker.score_expanded(query, self.sub_docs[x])\n",
    "                sub_vecs[x]=sub_vec\n",
    "            \n",
    "        self.subdoc_scores = sub_vecs\n",
    "        \n",
    "        self.sub_list_scores=[]\n",
    "        for x in self.subdoc_scores.keys():\n",
    "            for y in self.subdoc_scores[x]:\n",
    "                self.sub_list_scores.append(y)\n",
    "    \n",
    "    def rank_relevant(self, ranker, expanded=True):\n",
    "        sub_vecs={}\n",
    "        \n",
    "        if expanded==False:\n",
    "            query=self.prime_q\n",
    "            for x in self.relevant_set.keys():\n",
    "                sub_vec = ranker.score(query, self.relevant_set[x])\n",
    "                sub_vecs[x]=sub_vec\n",
    "        else:\n",
    "            query=self.expanded_q\n",
    "            for x in self.relevant_set.keys():\n",
    "                sub_vec = ranker.score_expanded(query, self.relevant_set[x])\n",
    "                sub_vecs[x]=sub_vec\n",
    "            \n",
    "        self.relevant_scores = sub_vecs\n",
    "        \n",
    "        self.rel_list_scores=[]\n",
    "        for x in self.relevant_scores.keys():\n",
    "            for y in self.relevant_scores[x]:\n",
    "                self.rel_list_scores.append(y)\n",
    "       \n",
    "        \n",
    "    #******************************************************************************\n",
    "    #----------------------------Pruning Relevant Set------------------------------\n",
    "    #******************************************************************************\n",
    "    def prune_docs(self, method='finite', cutoff=0):\n",
    "        \n",
    "        '''\n",
    "        method percentile: cutoff is the percentile to lower bound the document scores on\n",
    "        method finite: cutoff is a hard value to cutoff scores on\n",
    "        \n",
    "        store the indexes of the documents that have scores over the cutoff\n",
    "        these indexes will be used in the creation of the subdocs\n",
    "        '''\n",
    "        \n",
    "        if method=='percentile':\n",
    "            p = np.percentile(self.document_scores, cutoff)\n",
    "            cuts = np.where(self.document_scores<p)\n",
    "            \n",
    "            for x in range(0, len(self.documents)):\n",
    "                if x not in cuts:\n",
    "                    self.pruned_docs.append(x)\n",
    "                                                 \n",
    "        elif method=='finite':\n",
    "            for x in range(0, len(self.documents)):\n",
    "                if self.document_scores[x]>cutoff:\n",
    "                    self.pruned_docs.append(x)\n",
    "                    \n",
    "            \n",
    "    def prune_subdocs(self, method='finite', cutoff=0):\n",
    "        \n",
    "        '''\n",
    "        method percentile: cutoff is the percentile to lower bound the document scores on\n",
    "        method finite: cutoff is a hard value to cutoff scores on\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if method=='percentile':\n",
    "            p = np.percentile(self.sub_list_scores, cutoff)\n",
    "            prune={}\n",
    "            for x in self.sub_docs.keys():\n",
    "                for y in range(0, len(sub_docs[x])):\n",
    "                    if subdoc_scores[x][y]>p:\n",
    "                        if x not in prune.keys():\n",
    "                            prune[x]=[y]\n",
    "                        else:\n",
    "                            prune[x].append(y)\n",
    "            self.pruned_subdocs=prune\n",
    "            \n",
    "        elif method=='finite':\n",
    "            prune={}\n",
    "            for x in self.sub_docs.keys():\n",
    "                for y in range(0, len(self.sub_docs[x])):\n",
    "                    if self.subdoc_scores[x][y]>cutoff:\n",
    "                        if x not in prune.keys():\n",
    "                            prune[x]=[y]\n",
    "                        else:\n",
    "                            prune[x].append(y)\n",
    "            \n",
    "            self.pruned_subdocs=prune\n",
    "        \n",
    "    def make_relevant(self):\n",
    "        for x in self.pruned_subdocs.keys():\n",
    "            self.relevant_set[x]=[]\n",
    "            self.relevant_scores[x]=[]\n",
    "            for y in self.pruned_subdocs[x]:\n",
    "                self.relevant_set[x].append(self.sub_docs[x][y])\n",
    "                self.relevant_scores[x].append(self.subdoc_scores[x][y])\n",
    "        \n",
    "        self.rel_list=[]\n",
    "        for x in self.relevant_set.keys():\n",
    "            for y in self.relevant_set[x]:\n",
    "                self.rel_list.append(y)\n",
    "        \n",
    "    def prune_relevant(self, method='percentile',cutoff=15):\n",
    "        #Relevant is as low as it goes, these will be adjusted directly when pruned\n",
    "        \n",
    "        if method=='percentile':\n",
    "            cut = np.percentile(self.rel_list_scores, cutoff)\n",
    "        else:\n",
    "            cut = cutoff\n",
    "        \n",
    "        subbed_data = self.relevant_set\n",
    "        sub_scores = self.relevant_scores\n",
    "        \n",
    "        for x in self.sub_docs.keys():\n",
    "\n",
    "            subbed_data[x] = [xv if c else None for c, xv in zip(sub_scores[x]>cut, subbed_data[x])]\n",
    "            subbed_data[x] = [y for y in subbed_data[x] if y!=None]\n",
    "            sub_scores[x] = [y for y in sub_scores[x] if y>cut]\n",
    "        \n",
    "        self.relevant_set = {k: v for k, v in subbed_data.items() if len(v) > 0}\n",
    "        self.relevant_scores={k: v for k, v in sub_scores.items() if len(v) > 0}\n",
    "        \n",
    "        self.rel_list=[]\n",
    "        for x in self.relevant_set.keys():\n",
    "            for y in self.relevant_set[x]:\n",
    "                self.rel_list.append(y)\n",
    "        \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull api keys from the config file\n",
    "cfig=config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a ticker object \n",
    "tick = Ticker(cfig, \"F\",source='yahoo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.80051\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "wq=web_query(cfig)\n",
    "testq=tick.name\n",
    "\n",
    "d_start=\"11/1/2021\"\n",
    "#query all of the news apis in web_query object\n",
    "wq.query_all(query=tick.name, ticker=tick.ticker, d_start=d_start, threaded=False)\n",
    "#compile results into a singular dataframe\n",
    "wq.compile_results()\n",
    "\n",
    "end = timeit.default_timer()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3e34f21e3b497f9b7837e1b048ae62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=200)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.910349000000004\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "#scrap text from the results urls to form documents\n",
    "wq.scrape_results(threaded=True, max_docs=200)\n",
    "\n",
    "end = timeit.default_timer()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wq.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build corpus from web query results\n",
    "corpus=Corpus()\n",
    "#store the web query data frame in the corpus for referencing urls and titles\n",
    "corpus.set_results(df)\n",
    "#assign corpus documents as the web query documents\n",
    "corpus.set_corpus(wq.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:28884\n",
      "Number of documents:189\n"
     ]
    }
   ],
   "source": [
    "#pull in stop words and build corpus vocabulary \n",
    "stopwords=[]\n",
    "with open('util/stopwords.txt') as f:\n",
    "    stopwords.append(f.read().splitlines())\n",
    "stopwords=stopwords[0]\n",
    "\n",
    "corpus.build_vocabulary(stopwords)\n",
    "\n",
    "print(\"Vocabulary size:\" + str(len(corpus.vocabulary)))\n",
    "print(\"Number of documents:\" + str(len(corpus.documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25(norm='l2', smooth_idf=True, stopwords=stopwords, sublinear_tf=True, vocabulary=corpus.vocabulary)\n",
    "bm25.fit(corpus.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.build_queries(ticker=tick)\n",
    "corpus.rank_docs(ranker=bm25)\n",
    "corpus.prune_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (805 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-docs: 1779\n",
      "Source count: 185\n",
      "9.585755299999988\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "#create the sub_documents, wrapper to run multiple functions\n",
    "#passing in the tokenizer to save a little on class dependencies\n",
    "\n",
    "corpus.sub_divide(tokenizer=tokenizer, cutoff=2, method='sen')\n",
    "print('Sub-docs:',len(corpus.sub_list))\n",
    "print('Source count:', len(corpus.sub_docs))\n",
    "\n",
    "end = timeit.default_timer()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.rank_subdocs(ranker=bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.prune_subdocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sub_docs: 1779\n",
      "Number zero ranked: 1251\n",
      "percent of useless subdocs: 70.32 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sub_docs:\",len(corpus.sub_list))\n",
    "t = np.array(corpus.sub_list_scores)\n",
    "z=np.where(t==0.0)\n",
    "print(\"Number zero ranked:\", len(z[0]))\n",
    "print(\"percent of useless subdocs:\", round((len(z[0])/len(corpus.sub_list))*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant sources: 177\n",
      "relevant sub_docs: 528\n"
     ]
    }
   ],
   "source": [
    "corpus.make_relevant()\n",
    "print('relevant sources:',len(corpus.relevant_set))\n",
    "print('relevant sub_docs:',len(corpus.rel_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.rank_relevant(ranker=bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "relevant_set = corpus.relevant_set\n",
    "relevant_scores = corpus.relevant_scores\n",
    "print(len(relevant_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b39e8bbb1d9430e86a4d4ad5bc4491c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=528)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run the new relevant set through distilled-BERT and get sentiment classifications\n",
    "sentiments = get_sentiments(relevant_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens=[]\n",
    "for x in relevant_set.keys():\n",
    "    for y in range(0, len(relevant_set[x])):\n",
    "        lens.append(len(relevant_set[x][y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "avlen=np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_weight={}\n",
    "for x in relevant_set.keys():\n",
    "    l=[]\n",
    "    for y in range(0, len(relevant_set[x])):\n",
    "        l.append(len(relevant_set[x][y])/avlen)\n",
    "    len_weight[x]=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rel = {}\n",
    "for x in relevant_scores.keys():\n",
    "    l=[]\n",
    "    for y in range(0, len(relevant_scores[x])):\n",
    "        l.append(relevant_scores[x][y] * len_weight[x][y])\n",
    "    adjusted_rel[x]=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for x in relevant_set.keys():\n",
    "#     print(x, \"relevance:\", relevant_scores[x])\n",
    "#     print(\"  adjusted r:\", adjusted_rel[x])\n",
    "#     print(\"  sentiments:\", sentiments[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_scores=[]\n",
    "lrw_scores=[]\n",
    "for x in relevant_scores.keys():\n",
    "    for y in range(0, len(relevant_scores[x])):\n",
    "        rw_scores.append(relevant_scores[x][y] * sentiments[x][y])\n",
    "        lrw_scores.append(adjusted_rel[x][y] * sentiments[x][y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Relevance weighted Sentiment: -0.0944\n",
      "Average Length adjusted Relevance weighted sentiment: -0.1218\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Relevance weighted Sentiment:\", np.mean(rw_scores).round(4))\n",
    "print(\"Average Length adjusted Relevance weighted sentiment:\", np.mean(lrw_scores).round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
