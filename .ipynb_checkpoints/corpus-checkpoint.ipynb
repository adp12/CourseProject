{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dateutil import parser as dparser\n",
    "import urllib.parse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from util.config import config\n",
    "from util.pyBM25 import BM25\n",
    "from util.pyRanker2 import BM25 as BM25_QE\n",
    "from util.web_query import web_query\n",
    "from util.ticker import Ticker\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "classifier = pipeline(task='sentiment-analysis',model=model,tokenizer=tokenizer)\n",
    "max_tokens = int(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running through sub_docs and outputting sentiment list\n",
    "\n",
    "def get_sentiments(docs):\n",
    "    \n",
    "    c = 0\n",
    "    for x in docs.keys():\n",
    "        c+=len(docs[x])\n",
    "    \n",
    "    pgres = widgets.IntProgress(value=0,min=0,max=c, step=1)\n",
    "    display(pgres)\n",
    "    \n",
    "    sentiments = {}\n",
    "    \n",
    "    for x in docs.keys():\n",
    "        scrs=[]\n",
    "        for y in range(0, len(docs[x])):\n",
    "            \n",
    "            s = classifier(docs[x][y])\n",
    "            scr = s[0]['score']\n",
    "            if s[0]['label']==\"NEGATIVE\":\n",
    "                scr=scr*-1\n",
    "            scrs.append(scr)\n",
    "            pgres.value+=1\n",
    "            pgres.description=str(pgres.value)+\":\"+str(c)\n",
    "        \n",
    "        sentiments[x]=scrs\n",
    "                \n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the rows of a 2d input_matrix so they sum to 1\n",
    "    \"\"\"\n",
    "\n",
    "    row_sums = input_matrix.sum(axis=1)\n",
    "    try:\n",
    "        assert (np.count_nonzero(row_sums)==np.shape(row_sums)[0]) # no row should sum to zero\n",
    "    except Exception:\n",
    "        raise Exception(\"Error while normalizing. Row(s) sum to zero\")\n",
    "    new_matrix = input_matrix / row_sums[:, np.newaxis]\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    #******************************************************************************\n",
    "    #----------------------------------Method---------------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    #Run the web_query to produce a collection of text documents scraped from the web\n",
    "    \n",
    "    #Use the set_results() function to store the full results in the corpus for processing\n",
    "    \n",
    "    #Use the set_corpus() function to assign the documents scraped from the web to the corpus\n",
    "    \n",
    "    #Sub divide the documents into smaller sub_docs\n",
    "    \n",
    "    #Rank the documents based on relevance to the original query as well as any tags\n",
    "    \n",
    "    #Prune the sub_docs to produce a relevant set\n",
    "    \n",
    "    #\n",
    "    \n",
    "    #******************************************************************************\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #typical corpus data\n",
    "        self.documents = []\n",
    "        self.vocabulary = []\n",
    "        self.number_of_documents = 0\n",
    "        self.vocabulary_size = 0\n",
    "        \n",
    "        #plsa and liklihoods\n",
    "        self.likelihoods = []\n",
    "        self.term_doc_matrix = None \n",
    "        self.document_topic_prob = None  # P(z | d)\n",
    "        self.topic_word_prob = None  # P(w | z)\n",
    "        self.topic_prob = None  # P(z | d, w)\n",
    "                \n",
    "        #for web results\n",
    "        self.query_results=None\n",
    "        self.max_tokens=512\n",
    "        self.failed = []\n",
    "        \n",
    "        #sub dividing documents\n",
    "        self.tokenizer=None\n",
    "        self.sub_docs=None\n",
    "        \n",
    "        #relevance scores\n",
    "        self.document_scores=None\n",
    "        self.document_tag_scores=None\n",
    "        self.subdoc_scores=None\n",
    "        self.subdoc_tag_scores=None\n",
    "        self.title_scores=None\n",
    "                \n",
    "        #pruned data\n",
    "        self.relevant_set=None\n",
    "        self.relevant_scores=None\n",
    "    \n",
    "    \n",
    "    #******************************************************************************\n",
    "    #------------------------------Setting Corpus----------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def set_results(self, df):\n",
    "        #dataframe returned from webquery\n",
    "        self.query_results=df\n",
    "    \n",
    "    def set_corpus(self, documents):\n",
    "        self.documents = documents\n",
    "        \n",
    "    def build_corpus_from_url(self, max_docs=50):\n",
    "        \n",
    "        #scrape text from url-list to build corpus\n",
    "        #(not recommended, use the same method from the web_query object and the set_corpus() method)\n",
    "        \n",
    "        url_list = self.query_results['url'].tolist()\n",
    "        url_list = url_list[0:max_docs]\n",
    "        \n",
    "        pgres = widgets.IntProgress(value=0,min=0,max=len(url_list), step=1)\n",
    "        display(pgres)\n",
    "        \n",
    "        failed=[]\n",
    "        headers = {\"User-Agent\":\"Mozilla/5.0\"}\n",
    "        for i in range(0,len(url_list)):\n",
    "            try:\n",
    "                response = requests.get(url=url_list[i],headers=headers)\n",
    "                if response.status_code==200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    d = soup.get_text()\n",
    "                    if len(d)>200:\n",
    "                        self.documents.append(d)\n",
    "                else:\n",
    "                    self.failed.append(i)\n",
    "            except:\n",
    "                self.failed.append(i)\n",
    "\n",
    "            finally:\n",
    "                pgres.value+=1\n",
    "                pgres.description=str(i+1)+\":\"+str(len(url_list))\n",
    "                \n",
    "        self.number_of_documents=len(self.documents)\n",
    "        #remove failed url responses from dataset\n",
    "        self.query_results = self.query_results.take(list(set(range(self.query_results.shape[0]))-set(self.failed)))\n",
    "        \n",
    "\n",
    "    def build_corpus_from_file(self, file_path):\n",
    "\n",
    "        f = open(file_path, 'r')\n",
    "        docs = f.readlines()\n",
    "        for d in docs:\n",
    "            self.documents.append(d)\n",
    "        self.number_of_documents = len(docs)\n",
    "\n",
    "        \n",
    "    def build_vocabulary(self, stopwords):\n",
    "\n",
    "        v = set([])\n",
    "        for x in self.documents:\n",
    "            tmp = set(x.split())\n",
    "            tmp = {x for x in tmp if x.lower() not in stopwords}\n",
    "                        \n",
    "            v.update(tmp)\n",
    "        \n",
    "        v = list(v)\n",
    "        self.vocabulary = v\n",
    "        self.vocabulary_size = len(v)\n",
    "        \n",
    "             \n",
    "    \n",
    "    #******************************************************************************\n",
    "    #------------------------------Sub Dividing-------------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def get_pgraphs(self, doc, cutoff, method):\n",
    "        #updated get_pgraphs() with method for cutoff\n",
    "        #cut off method:\n",
    "        #sen: number of sentences\n",
    "        #word: number of words  \n",
    "\n",
    "        pgraphs=[]\n",
    "        freshsoup = re.split('\\n\\n',doc)\n",
    "        for x in range(0,len(freshsoup)):\n",
    "            if method=='word':\n",
    "                words = len(str(freshsoup[x]).strip().split(' ',maxsplit=cutoff))\n",
    "                if words>cutoff:\n",
    "                    pgraphs.append(freshsoup[x])\n",
    "                    \n",
    "            elif method=='sen':\n",
    "                sens = len(re.findall(\"\\.\",str(freshsoup[x]).strip()))\n",
    "                if sens>cutoff:\n",
    "                    pgraphs.append(freshsoup[x])\n",
    "                    \n",
    "        return pgraphs\n",
    "    \n",
    "    def split_doc(self, doc, subs):         \n",
    "            \n",
    "        if len(re.findall(r'\\.', doc))>1:\n",
    "            cut_point = doc.rfind('.', 0, int(len(doc)/2))+1\n",
    "        else:\n",
    "            cut_point = int(len(doc)/2)\n",
    "\n",
    "        d1 = doc[0:cut_point]\n",
    "        d2 = doc[cut_point+1:]\n",
    "\n",
    "        tkns1 = int(len(self.tokenizer(d1)['input_ids']))\n",
    "\n",
    "        if tkns1>self.max_tokens:\n",
    "            self.split_doc(d1,subs)\n",
    "        else:\n",
    "            if len(d1)>0:\n",
    "                subs.append(d1)\n",
    "\n",
    "        tkns2 = int(len(self.tokenizer(d2)['input_ids']))\n",
    "\n",
    "        if tkns2>self.max_tokens:\n",
    "            self.split_doc(d2, subs)\n",
    "        else:\n",
    "            if len(d2)>0:\n",
    "                subs.append(d2)\n",
    "            \n",
    "    \n",
    "    def get_subdocs(self, pgraphs):\n",
    "        #Updated get_subdocs with iterative slicing \n",
    "        #ensure sub_docs tokens will not exceed max_tokens for sentiment model\n",
    "        sub_docs=[]\n",
    "\n",
    "        for x in range(0, len(pgraphs)):\n",
    "            sen_cnt = len(re.split('\\n|\\. ',pgraphs[x]))\n",
    "            tkns = int(len(tokenizer(pgraphs[x])['input_ids']))\n",
    "\n",
    "            if tkns<self.max_tokens:\n",
    "                sub_docs.append(pgraphs[x])\n",
    "            else:\n",
    "                self.split_doc(pgraphs[x],sub_docs)\n",
    "        \n",
    "        return sub_docs\n",
    "        \n",
    "    def sub_divide(self, tokenizer, cutoff=1, method='sen'):\n",
    "\n",
    "        #creates a dictionary of sub_docs divided from each document in the corpus\n",
    "        #method: using get_pgraphs() followed by get_subdocs()\n",
    "        #output form: dict{ document_id : [subdoc_1, subdoc_2 ... subdoc_n] }\n",
    "\n",
    "        subbed_data = {}\n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "        for x in range(0, len(self.documents)):\n",
    "\n",
    "            pg = self.get_pgraphs(self.documents[x], cutoff, method)\n",
    "            subs = self.get_subdocs(pg)\n",
    "            subbed_data[x]=subs\n",
    "\n",
    "        self.sub_docs = subbed_data\n",
    "\n",
    "        \n",
    "    #******************************************************************************\n",
    "    #----------------------------------Relevance Scoring---------------------------\n",
    "    #******************************************************************************  \n",
    "    \n",
    "    def rank_docs(self, query, ranker):\n",
    "        self.document_scores = ranker.score(query, self.documents)\n",
    "        \n",
    "    def rank_doc_tags(self, tags, ranker):\n",
    "        tag_scores=[]\n",
    "        for t in tags:\n",
    "            scores = ranker.score(t, self.documents)\n",
    "            tag_scores.append(scores)\n",
    "            \n",
    "        self.document_tag_scores = tag_scores\n",
    "        \n",
    "    def rank_subdocs(self, query, ranker):\n",
    "        sub_vecs={}\n",
    "        for x in self.sub_docs.keys():\n",
    "            sub_vec = ranker.score(query, self.sub_docs[x])\n",
    "            sub_vecs[x]=sub_vec\n",
    "            \n",
    "        self.subdoc_scores = sub_vecs\n",
    "    \n",
    "    def rank_subdocs_tags(self, tags, ranker):\n",
    "        \n",
    "        tag_scores=[]\n",
    "        for t in tags:\n",
    "            sub_vecs={}\n",
    "            for x in self.sub_docs.keys():\n",
    "                sub_vec = ranker.score(t, self.sub_docs[x])\n",
    "                sub_vecs[x]=sub_vec\n",
    "            tag_scores.append(sub_vecs)\n",
    "        \n",
    "        self.subdoc_tag_scores = tag_scores\n",
    "    \n",
    "    def rank_titles(self, name, ranker):\n",
    "        name = re.sub('(,|\\.|Inc| )',\"\",str(name))\n",
    "        titles = self.query_results['title'].tolist()\n",
    "        self.title_scores = ranker.score(name, titles)\n",
    "        \n",
    "    def rank_ticker(self, ticker, ranker):\n",
    "        \n",
    "        #Takes a ticker object and runs all of the rankers above\n",
    "        \n",
    "        name = ticker.name\n",
    "        sym = ticker.ticker\n",
    "        tags = ticker.tags\n",
    "        \n",
    "        self.rank_docs(name,ranker)\n",
    "        self.rank_doc_tags(tags, ranker)\n",
    "        self.rank_subdocs(name,ranker)\n",
    "        self.rank_subdocs_tags(tags,ranker)\n",
    "        self.rank_titles(name,ranker)\n",
    "        \n",
    "    #******************************************************************************\n",
    "    #----------------------------Pruning Relevant Set------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def prune_subdocs(self, cutoff=0.4):\n",
    "        subbed_data = self.sub_docs\n",
    "        sub_scores = self.subdoc_scores\n",
    "        for x in self.sub_docs.keys():\n",
    "\n",
    "            subbed_data[x] = [xv if c else None for c, xv in zip(sub_scores[x]>cutoff, subbed_data[x])]\n",
    "            subbed_data[x] = [y for y in subbed_data[x] if y!=None]\n",
    "            sub_scores[x] = [y for y in sub_scores[x] if y>cutoff]\n",
    "        \n",
    "        self.relevant_set = {k: v for k, v in subbed_data.items() if len(v) > 0}\n",
    "        self.relevant_scores={k: v for k, v in sub_scores.items() if len(v) > 0}\n",
    "    \n",
    "    #******************************************************************************\n",
    "    #-------------------------------------PLSA (from MP3)--------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def build_term_doc_matrix(self):\n",
    "        \n",
    "        m = []\n",
    "        line = []\n",
    "        for x in self.documents:\n",
    "            doc = list(x.split())\n",
    "            for itm in self.vocabulary:\n",
    "                line.append(x.count(itm))\n",
    "            m.append(line)\n",
    "            line = []\n",
    "        self.term_doc_matrix = np.array(m)\n",
    "        \n",
    "    def initialize_prob(self, number_of_topics):\n",
    "\n",
    "        self.document_topic_prob = np.random.random_sample((self.number_of_documents, number_of_topics))\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "\n",
    "        self.topic_word_prob = np.random.random_sample((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "\n",
    "            \n",
    "    def E_step(self):\n",
    "        \n",
    "        for x in range(0,self.term_doc_matrix.shape[0]):  #loop through documents\n",
    "            e = self.document_topic_prob[x].reshape(-1,1)*self.topic_word_prob\n",
    "            self.topic_prob[x] = normalize(e)\n",
    "           \n",
    "\n",
    "    def M_step(self, number_of_topics):\n",
    "        \n",
    "        pz = []\n",
    "        for x in range(0, self.term_doc_matrix.shape[0]):         \n",
    "            m = self.topic_prob[x]*self.term_doc_matrix[x].reshape(1,-1)\n",
    "            self.document_topic_prob[x] = np.sum(m,axis=1)\n",
    "            pz.append(m)\n",
    "\n",
    "        #update\n",
    "        \n",
    "        pz = np.array(pz)\n",
    "        self.topic_word_prob = np.sum(pz,axis=0)\n",
    "        \n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    " \n",
    "\n",
    "    def calculate_likelihood(self, number_of_topics):\n",
    "\n",
    "        l = np.log(np.prod(np.power(np.dot(self.document_topic_prob,self.topic_word_prob),self.term_doc_matrix),axis=1))\n",
    "        l = l[np.argmax(l)]\n",
    "        self.likelihoods.append(l)\n",
    "        \n",
    "\n",
    "    def plsa(self, number_of_topics, max_iter, epsilon):\n",
    "\n",
    "        self.build_term_doc_matrix()\n",
    "        self.topic_prob = np.zeros([self.number_of_documents, number_of_topics, self.vocabulary_size], dtype=np.float)\n",
    "        self.initialize_prob(number_of_topics)\n",
    "        current_likelihood = 0.0\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "            self.E_step()\n",
    "            self.M_step(number_of_topics)\n",
    "            \n",
    "            l = self.calculate_likelihood(number_of_topics)\n",
    "            \n",
    "            if current_likelihood==0 or current_likelihood==None or l>current_likelihood:\n",
    "                current_likelihood = l\n",
    "            else:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull api keys from the config file\n",
    "cfig=config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a ticker object \n",
    "tick = Ticker(cfig, \"AAPL\",source='yahoo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq=web_query(cfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10390db7112c46cc8e411d4492c85665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=200)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testq=tick.name\n",
    "d_start=\"11/1/2021\"\n",
    "#query all of the news apis in web_query object\n",
    "wq.query_all(query=tick.name, ticker=tick.ticker, d_start=d_start)\n",
    "#compile results into a singular dataframe\n",
    "wq.compile_results()\n",
    "#scrap text from the results urls to form documents\n",
    "wq.scrape_results(threaded=True, max_docs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wq.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build corpus from web query results\n",
    "corpus=Corpus()\n",
    "#store the web query data frame in the corpus for referencing urls and titles\n",
    "corpus.set_results(df)\n",
    "#assign corpus documents as the web query documents\n",
    "corpus.set_corpus(wq.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:27380\n",
      "Number of documents:189\n"
     ]
    }
   ],
   "source": [
    "#pull in stop words and build corpus vocabulary \n",
    "stopwords=[]\n",
    "with open('util/stopwords.txt') as f:\n",
    "    stopwords.append(f.read().splitlines())\n",
    "stopwords=stopwords[0]\n",
    "\n",
    "corpus.build_vocabulary(stopwords)\n",
    "\n",
    "print(\"Vocabulary size:\" + str(len(corpus.vocabulary)))\n",
    "print(\"Number of documents:\" + str(len(corpus.documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Technology', 'Consumer Electronics']\n"
     ]
    }
   ],
   "source": [
    "print(tick.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25q = BM25_QE(norm='l2', smooth_idf=True, stopwords=stopwords, sublinear_tf=True, vocabulary=corpus.vocabulary)\n",
    "bm25q.fit(corpus.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build BM25 ranker fit to the corpus vocabulary\n",
    "bm25 = BM25(norm='l2', smooth_idf=True, stopwords=stopwords, sublinear_tf=True, vocabulary=corpus.vocabulary)\n",
    "bm25.fit(corpus.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdoc = corpus.documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "testq = tick.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Apple Inc.', 1], ['Technology', 0.05], ['Consumer Electronics', 0.05]]\n"
     ]
    }
   ],
   "source": [
    "testqc = [[tick.name, 1]]\n",
    "for q in tick.tags:\n",
    "    testqc.append([q, 0.05])\n",
    "print(testqc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=0.8\n",
    "k1=1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(norm='l2', smooth_idf=True, stop_words=stopwords, sublinear_tf=True, vocabulary=corpus.vocabulary)\n",
    "cvec = CountVectorizer(stop_words=stopwords, vocabulary=corpus.vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corpus.documents\n",
    "cX = corpus.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec.fit(X)\n",
    "cvec.fit(X)\n",
    "vX = tvec.transform(X)\n",
    "cX = cvec.transform(cX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943.3193717277487\n"
     ]
    }
   ],
   "source": [
    "avdl = cX.sum(1).mean()\n",
    "print(avdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = cX.sum(1).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply CountVectorizer\n",
    "qw=[]\n",
    "tfc=[]\n",
    "Eidf=[]\n",
    "scr=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8890)\t1\n",
      "  (0, 17902)\t1\n"
     ]
    }
   ],
   "source": [
    "q = testqc[2]\n",
    "qe, = cvec.transform([q[0]])\n",
    "print(qe)\n",
    "assert sparse.isspmatrix_csr(qe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 2)\n"
     ]
    }
   ],
   "source": [
    "tf = cX.tocsc()[:,qe.indices]\n",
    "wtf = cX.tocsc()[:,qe.indices] * q[1]\n",
    "print(wtf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57536414 2.31305639]]\n"
     ]
    }
   ],
   "source": [
    "idf = tvec._tfidf.idf_[None, qe.indices] - 1.\n",
    "print(idf)\n",
    "# idf = idf[0]\n",
    "# print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 2)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(wtf.shape)\n",
    "print(idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T = tf.multiply(np.broadcast_to(idf, tf.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "(191, 2)\n",
      "0.7479009638497243\n",
      "0.7479009638497243\n"
     ]
    }
   ],
   "source": [
    "#t1 = (1/np.sum(wtf))\n",
    "t1 = (1/wtf.sum())\n",
    "print(t1.shape)\n",
    "t2 = wtf.multiply(np.broadcast_to(idf, wtf.shape))\n",
    "print(t2.shape)\n",
    "eidf = (t1*t2).sum()\n",
    "print(eidf)\n",
    "eidf = ((1/wtf.sum()) * wtf.multiply(np.broadcast_to(idf, wtf.shape))).sum()\n",
    "print(eidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Apple Inc.', 20], ['Technology', 0.01], ['Consumer Electronics', 0.01], ['investing', 0.01], ['analysis', 0.1]]\n"
     ]
    }
   ],
   "source": [
    "testqc = [[tick.name, 20]]\n",
    "for q in tick.tags:\n",
    "    testqc.append([q, 0.01])\n",
    "testqc.append(['investing',0.01])\n",
    "testqc.append(['analysis',0.1])\n",
    "print(testqc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple Inc.', 20]\n",
      "['Technology', 0.01]\n",
      "['Consumer Electronics', 0.01]\n",
      "['investing', 0.01]\n",
      "['analysis', 0.1]\n",
      "[0.015748356968139143, 0.08701137698962969, 0.7479009638497242, 0.22705745063534594, 1.791759469228055]\n"
     ]
    }
   ],
   "source": [
    "tfc=[]\n",
    "Eidf=[]\n",
    "for q in testqc:\n",
    "    print(q)\n",
    "    qe, = cvec.transform([q[0]])\n",
    "    tf = cX.tocsc()[:,qe.indices]\n",
    "    wtf = cX.tocsc()[:,qe.indices] * q[1]\n",
    "    tfc.append(tf)\n",
    "    idf = tvec._tfidf.idf_[None, qe.indices] - 1.\n",
    "    eidf = ((1/wtf.sum()) * wtf.multiply(np.broadcast_to(idf, wtf.shape))).sum()\n",
    "    \n",
    "    Eidf.append(eidf)\n",
    "print(Eidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr=[]\n",
    "for x in range(0, len(Eidf)):\n",
    "    denom = tfc[x] + (k1 * (1 - b + b * (dl/avdl)))[:,None]\n",
    "    numer = tfc[x].multiply(np.broadcast_to(Eidf[x], tfc[x].shape))\n",
    "    scr.append((numer/denom).sum(1).A1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfc=[]\n",
    "Eidf=[]\n",
    "for q in testqc:\n",
    "    qe, = cvec.transform([q[0]])\n",
    "    tf = cX.tocsc()[:,qe.indices]\n",
    "    wtf = cX.tocsc()[:,qe.indices] * q[1]\n",
    "    tfc.append(tf)\n",
    "    idf = tvec._tfidf.idf_[None, qe.indices] - 1.\n",
    "    eidf = ((1/wtf.sum()) * wtf.multiply(np.broadcast_to(idf, wtf.shape))).sum()\n",
    "    \n",
    "    Eidf.append(eidf)\n",
    "\n",
    "scr=[]\n",
    "for x in range(0, len(Eidf)):\n",
    "    denom = tfc[x] + (k1 * (1 - b + b * (dl/avdl)))[:,None]\n",
    "    numer = tfc[x].multiply(np.broadcast_to(Eidf[x], tfc[x].shape))\n",
    "    scr.append((numer/denom).sum(1).A1)\n",
    "\n",
    "output_score = np.sum(scr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21961446 0.22209635 0.0917946  0.06931279 1.98550641 0.06331261\n",
      " 0.0615192  0.76522471 0.90929145 0.4982985  0.4391558  0.68877352\n",
      " 0.         0.0140211  0.         0.0882048  0.08564178 0.08793357\n",
      " 0.0384438  2.10700883 0.0151299  0.66995838 0.71035864 1.45605629\n",
      " 0.63428402 0.53256288 0.88177186 0.77661089 0.21087984 0.8188277\n",
      " 1.00738495 0.         0.44369899 0.01249274 0.01248224 0.58390788\n",
      " 1.12984333 0.25748005 1.50094353 0.23431583 0.76648242 0.48354665\n",
      " 0.21926998 0.74614116 1.9938945  0.78068717 0.69514201 0.25008704\n",
      " 0.07141475 0.72852924 0.20616076 0.71781011 0.91766218 0.09307619\n",
      " 0.19512356 0.70529882 1.69586766 1.49255266 0.07621664 0.22287237\n",
      " 0.22190744 0.52559617 0.82445124 0.17343774 0.15106811 0.06268624\n",
      " 0.07692104 0.61741173 0.22153941 0.71276376 2.0522942  0.84984066\n",
      " 0.70415523 0.43738289 0.23638181 0.05870399 1.02979744 0.98829101\n",
      " 0.71149943 0.96930008 0.81009357 0.16550989 1.11626577 0.86588088\n",
      " 0.68065102 1.44684752 1.67830822 0.2253744  0.75031476 0.76266063\n",
      " 0.73648368 0.15993259 0.73746113 0.08153461 0.66151348 0.20745438\n",
      " 1.70321908 0.4887168  0.72004466 0.2468015  0.80838274 0.20207861\n",
      " 0.79407063 0.8216894  0.69372715 1.8792891  1.12882855 0.81974903\n",
      " 0.21885433 0.72390758 0.21372805 0.7246157  1.66068311 0.81891705\n",
      " 0.01534809 0.57238079 0.19974817 0.77278152 0.55897854 0.59151754\n",
      " 0.67832396 0.75433928 0.68379289 1.27310358 0.75632707 0.5881584\n",
      " 0.16146919 0.7100495  1.33608934 0.25525243 0.77761877 0.52431457\n",
      " 1.68198203 0.65692337 0.21836326 0.59151754 0.55373009 0.01250762\n",
      " 0.82038643 0.62825319 0.87750836 2.12327291 0.729426   0.58443261\n",
      " 0.71475063 0.71019752 0.81331895 0.68989599 1.38345088 1.47954232\n",
      " 0.72807168 1.46897171 1.17432394 0.41737607 0.22320518 1.71057534\n",
      " 1.60309757 1.56375607 0.22304639 0.19971257 0.25096947 1.65688583\n",
      " 0.57031583 0.75768323 2.07507031 0.58983233 0.62912441 0.58281889\n",
      " 1.06804161 0.07897846 0.06364152 1.11939609 0.7082508  0.23669884\n",
      " 0.21197889 0.49759246 0.59427733 0.53554474 0.23800592 0.87916039\n",
      " 0.73995283 0.71888708 0.68520068 0.72760117 1.03448441 1.60851511\n",
      " 0.7063429  0.70597342 1.00680086 0.77150794 0.20114828]\n",
      "141\n",
      "2.123272909487606\n",
      "(array([12, 14, 31], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testscr = np.sum(scr, axis=0)\n",
    "print(testscr)\n",
    "print(np.argmax(testscr))\n",
    "print(testscr[np.argmax(testscr)])\n",
    "print(np.where(testscr==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Profit Margins Make Difference On Earnings From Lowe’s, Target | Investing.com\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Breaking News\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Black Friday SALE: Up to 54% off InvestingPro!\n",
      "Register here\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Quotes\n",
      "\n",
      "\n",
      "All Instrument Types\n",
      "\n",
      "\n",
      "\n",
      "All Instrument TypesIndicesEquitiesETFsFundsCommoditiesCurrenciesCryptoBondsCertificates \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please try another search\n",
      "\n",
      "\n",
      "\n",
      "Search website for: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Popular News\n",
      "More\n",
      "\n",
      "\n",
      " \n",
      "Risk assets plunge as virus fears cause post-Thanksgiving blues\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Oil plunges $10/bbl on new coronavirus variant concerns\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Wall Street Opens Sharply Lower as Covid Concerns Flare Again; Dow Down 800 Pts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Popular Analysis\n",
      "More\n",
      "\n",
      "\n",
      " \n",
      "Zoom Stock’s 38% Plunge Makes It An Attractive Buy\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Is The NASDAQ Composite Running On Empty?\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "3 Stocks Poised For New Highs As Fed Rate Hikes Expected Sooner Than Anticipated\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Sign In/Free Sign Up \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "Recent Alerts\n",
      "\n",
      "\n",
      "Sign up to create alerts for Instruments,\n",
      "Economic Events and content by followed authors\n",
      "Free Sign Up Already have an account? Sign In\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "English (UK)РусскийEnglish (India)TürkçeEnglish (Canada)‏العربية‏English (Australia)ΕλληνικάEnglish (South Africa)SvenskaEnglish (Philippines)SuomiEnglish (Nigeria)עבריתDeutsch日本語Español (España)한국어Español (México)简体中文Français繁體中文ItalianoBahasa IndonesiaNederlandsBahasa MelayuPortuguês (Portugal)ไทยPolskiTiếng ViệtPortuguês (Brasil)हिंदी \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Markets\n",
      "\n",
      "\n",
      "Indices\n",
      "\n",
      "\n",
      "Indices Futures\n",
      "Major Indices\n",
      "Indices CFDs\n",
      "World Indices\n",
      "Global Indices \n",
      "\n",
      "Dow Jones Futures\n",
      "S&P 500 Futures\n",
      "Nasdaq Futures\n",
      "S&P 500\n",
      "Dow Jones\n",
      "Nasdaq 100\n",
      "DAX\n",
      "FTSE 100\n",
      "Euro Stoxx 50\n",
      "Nikkei 225\n",
      "\n",
      " \n",
      "\n",
      "Stocks\n",
      "\n",
      "\n",
      "Stock Screener\n",
      "Trending Stocks\n",
      "United States\n",
      "Pre-Market\n",
      "Earnings Calendar\n",
      "Americas\n",
      "Europe\n",
      "52 Week High\n",
      "52 Week Low\n",
      "Most Active\n",
      "Top Gainers\n",
      "Top Losers\n",
      "World ADRs\n",
      "Marijuana Stocks \n",
      "\n",
      "Apple\n",
      "Tesla\n",
      "Meta Platforms\n",
      "Amazon.com\n",
      "Microsoft\n",
      "NVIDIA\n",
      "Netflix\n",
      "Pfizer\n",
      "AMD\n",
      "Boeing\n",
      "GameStop Corp\n",
      "AMC Entertainment\n",
      "Nio A ADR\n",
      "\n",
      " \n",
      "\n",
      "Commodities\n",
      "\n",
      "\n",
      "Real Time Commodities\n",
      "Metals\n",
      "Energy\n",
      "Grains\n",
      "Softs\n",
      "Meats\n",
      "Commodity Indices \n",
      "\n",
      "Gold\n",
      "Crude Oil WTI\n",
      "Brent Oil\n",
      "Silver\n",
      "Natural Gas\n",
      "Copper\n",
      "US Wheat\n",
      "\n",
      " \n",
      "\n",
      "Cryptocurrency\n",
      "\n",
      "\n",
      "All Cryptocurrencies\n",
      "Cryptocurrency Pairs\n",
      "ICO Calendar\n",
      "Bitcoin\n",
      "Ethereum\n",
      "Cardano\n",
      "Solana\n",
      "Dogecoin\n",
      "SHIBA INU\n",
      "Currency Converter \n",
      "\n",
      "BTC/USD\n",
      "ETH/USD\n",
      "BCH/USD\n",
      "LTC/USD\n",
      "DOGE/USD\n",
      "ETC/USD\n",
      "ETH/BTC\n",
      "XRP/USD\n",
      "Bitcoin Futures CME\n",
      "\n",
      " \n",
      "\n",
      "Currencies\n",
      "\n",
      "\n",
      "Forex Rates\n",
      "Single Currency Crosses\n",
      "Live Currency Cross Rates\n",
      "Exchange Rates Table\n",
      "Forward Rates\n",
      "FX Futures\n",
      "FX Options \n",
      "\n",
      "EUR/USD\n",
      "GBP/USD\n",
      "EUR/CHF\n",
      "AUD/USD\n",
      "USD/JPY\n",
      "USD/CAD\n",
      "USD/CHF\n",
      "USD/TRY\n",
      "USD/MXN\n",
      "BTC/USD\n",
      "Dollar Index\n",
      "\n",
      " \n",
      "\n",
      "ETFs\n",
      "\n",
      "\n",
      "World ETFs\n",
      "Major ETFs\n",
      "USA ETFs\n",
      "Marijuana ETFs \n",
      "\n",
      "SPDR S&P 500\n",
      "iShares MSCI Emerging Markets\n",
      "SPDR DJIA\n",
      "Invesco QQQ Trust\n",
      "\n",
      " \n",
      "\n",
      "Funds\n",
      "\n",
      "\n",
      "World Funds\n",
      "Major Funds \n",
      "\n",
      "Vanguard 500 Index Admiral\n",
      "Vanguard Total Bond Market II Index Fund Investor \n",
      "American Funds Capital Income Builder A\n",
      "PIMCO Commodity Real Return Strategy Institutional\n",
      "\n",
      " \n",
      "\n",
      "Bonds\n",
      "\n",
      "\n",
      "World Government Bonds\n",
      "Financial Futures\n",
      "Government Bond Spreads\n",
      "Bond Indices\n",
      "Forward Rates \n",
      "\n",
      "U.S. 10Y\n",
      "U.S. 30Y\n",
      "U.S. 2Y\n",
      "U.S. 5Y\n",
      "U.S. 3M\n",
      "US 10Y T-Note\n",
      "US 30Y T-Bond\n",
      "Euro Bund\n",
      "\n",
      " \n",
      "\n",
      "Certificates\n",
      "\n",
      "\n",
      "Major Certificates\n",
      "World Certificates \n",
      "\n",
      "SG FTSE MIB Gross TR 5x Daily Short Strategy RT 18\n",
      "Vontobel 7X Long Fixed Lever on Natural Gas 8.06\n",
      "BNP Call 500.59 EUR AEX 31Dec99\n",
      "COMMERZBANK AG Put CAC FUT 05/17 31Dec99\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "My Watchlist \n",
      "\n",
      "\n",
      "\n",
      "Crypto \n",
      "\n",
      "\n",
      "Cryptocurrency\n",
      "All Cryptocurrencies\n",
      "Cryptocurrency Pairs\n",
      "ICO Calendar\n",
      "Cryptocurrency Brokers\n",
      "Cryptocurrency\n",
      "Bitcoin\n",
      "Ethereum\n",
      "Ripple\n",
      "Cardano\n",
      "Solana\n",
      "Dogecoin\n",
      "SHIBA INU\n",
      "Currency Converter \n",
      "\n",
      "More in Cryptocurrency\n",
      "BTC/USD\n",
      "ETH/USD\n",
      "LTC/USD\n",
      "ETC/USD\n",
      "ETH/BTC\n",
      "IOTA/USD\n",
      "XRP/USD\n",
      "Bitcoin Futures CME\n",
      "Bitcoin Futures CBOE\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "News \n",
      "\n",
      "\n",
      "Financial News\n",
      "Stock Markets\n",
      "Cryptocurrency\n",
      "Commodities\n",
      "Forex\n",
      "Economy\n",
      "Economic Indicators\n",
      "Technology \n",
      "\n",
      "More In News\n",
      "Latest\n",
      "Most Popular\n",
      "Economic Calendar\n",
      "Sports & General\n",
      "World\n",
      "Politics\n",
      "Coronavirus\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Analysis \n",
      "\n",
      "\n",
      "Analysis\n",
      "Market Overview\n",
      "Forex\n",
      "Stock Markets\n",
      "Commodities\n",
      "Bonds\n",
      "Cryptocurrency\n",
      "ETFs \n",
      "\n",
      "More In Opinion\n",
      "Most Popular\n",
      "Editor's Picks\n",
      "Comics\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Charts \n",
      "\n",
      "\n",
      "Real Time Charts\n",
      "Live Charts\n",
      "Forex Chart\n",
      "Futures Chart\n",
      "Stocks Chart\n",
      "Indices Chart\n",
      "Cryptocurrency Chart \n",
      "\n",
      "Interactive Forex Chart\n",
      "Interactive Futures Chart\n",
      "Interactive Indices Chart\n",
      "Interactive Stocks Chart\n",
      "Multiple Forex Charts\n",
      "Multiple Indices Charts\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Technical \n",
      "\n",
      "\n",
      "Tools\n",
      "Technical Summary\n",
      "Technical Analysis\n",
      "Pivot Points\n",
      "Moving Averages\n",
      "Indicators\n",
      "Candlestick Patterns \n",
      "\n",
      "More In Technical\n",
      "Candlestick Patterns\n",
      "Fibonacci Calculator\n",
      "Pivot Point Calculator\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Brokers \n",
      "\n",
      "\n",
      "Brokers\n",
      "Forex Brokers\n",
      "Cryptocurrency Brokers\n",
      "Stock Brokers\n",
      "Online Brokers \n",
      "\n",
      "More In Brokers\n",
      "CFD Brokers\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Tools \n",
      "\n",
      "\n",
      "Calendars\n",
      "Economic Calendar\n",
      "Holiday Calendar\n",
      "Earnings Calendar\n",
      "Dividend Calendar\n",
      "Splits Calendar\n",
      "IPO Calendar\n",
      "Futures Expiry Calendar\n",
      "Investment Tools\n",
      "Stock Screener\n",
      "Fed Rate Monitor Tool\n",
      "Currency Converter\n",
      "Fibonacci Calculator \n",
      "\n",
      "More In Tools\n",
      "Forex Correlation\n",
      "Pivot Point Calculator\n",
      "Profit Calculator\n",
      "Margin Calculator\n",
      "Currencies Heat Map\n",
      "Forex Volatility\n",
      "Forward Rates Calculator\n",
      "Mortgage Calculator\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Education \n",
      "\n",
      "\n",
      "Education\n",
      "Webinars\n",
      "Trading Guide\n",
      "Terms \n",
      "\n",
      "Events\n",
      "Conferences\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "More \n",
      "\n",
      "\n",
      "Alerts\n",
      "Webinars\n",
      "Conferences\n",
      "Trading Guide\n",
      "Central Banks\n",
      "Insights\n",
      "Ad-Free Version \n",
      "\n",
      "Webmaster Tools\n",
      "Broker Blacklist\n",
      "Sentiments Outlook\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Most Popular  \n",
      "\n",
      " Editor's Picks  \n",
      "\n",
      " Market Overview  \n",
      "\n",
      " Stock Markets  \n",
      "\n",
      " Forex  \n",
      "\n",
      " Commodities  \n",
      "\n",
      " Cryptocurrency  \n",
      "\n",
      " Bonds  \n",
      "\n",
      " ETFs  \n",
      "\n",
      " Comics  \n",
      "\n",
      " Webinars  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ad-Free Version. Upgrade your Investing.com experience. Save up to 40% More details\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Profit Margins Make Difference On Earnings From Lowe’s, Target\n",
      "\n",
      "By TD Ameritrade (JJ Kinahan)Stock MarketsNov 17, 2021 10:33AM ET\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "www.investing.com/analysis/profit-margins-make-difference-on-earnings-from-lowes-target-200608675\n",
      "\n",
      "Profit Margins Make Difference On Earnings From Lowe’s, Target\n",
      "By TD Ameritrade (JJ Kinahan)   |  Nov 17, 2021 10:33AM ET \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Saved. See Saved Items.\n",
      "\n",
      "\n",
      "\n",
      "This article has already been saved in your Saved Items\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TD Ameritrade \n",
      "\n",
      "Our Website\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Follow\n",
      " \n",
      "\n",
      "Retailers continue to attract attention from investors with more earnings announcements. Before Wednesday’s open, Lowe’s (NYSE:LOW) and Target (NYSE:TGT) both announced better-than-expected earnings and revenue. Similar performances occurred on Tuesday when Home Depot (NYSE:HD) rallied 5.7% on better-than-expected earnings, but Walmart (NYSE:WMT) fell 2.6% despite beating its earnings estimates.\n",
      "The difference between Lowe’s and Target appears to be in the profit margins. Lowe’s reported expanding margins because it’s able to pass on price increases to consumers and has also benefitted from falling lumber prices. However, Target is getting pinched on its margins and may have to try and pass on more costs on to consumers.\n",
      "Online retailer Amazon (NASDAQ:AMZN) said it would stop accepting Visa (NYSE:V) cards issued in the U.K. because of the higher fees. Visa (NYSE:V) 3% lower before the bell on the news but Amazon’s shares were relatively unchanged.\n",
      "In EV news, Lucid (NASDAQ:LCID) was up 4.65% in premarket trading building on yesterday’s 23.71%. Investors appeared to welcome Lucid’s 2022 production goals. The company’s market cap has now surpassed both General Motors (NYSE:GM) and Ford NYSE:F) despite only delivering a few hundred cars per year. Canoo Inc (NASDAQ:GOEV) was also up 17.5% before the bell adding to its Tuesday rally of 23.67%. Canoo hopes to start delivering cars in 2022 on a limited basis. However, Rivian (NASDAQ:RIVN) was down about 8% in premarket trading despite rising more than 15% on Tuesday. After Tuesday’s close, Rivian was up more than 70% from its IPO five days ago.\n",
      "In other market news, Credit Suisse (SIX:CSGN) analysts designated Microsoft (NASDAQ:MSFT) and Qualcomm (NASDAQ:QCOM) as “outperform” on Tuesday. The high rating helped prompt 1% and 7.9% rallies in the stocks respectively. Cloud growth and semiconductor demand are helping to prompt the higher ratings.MetaverseSpeaking of the cloud and semiconductors, these are two big elements of the metaverse. After Facebook changed its name to Meta (NASDAQ:FB) to reflect its commitment to being a part of the metaverse, many investors have asked themselves, “What is the metaverse?” The term metaverse comes from a 1992 novel by Neal Stephenson called Snow Crash where the people could safely meet in an electronically shared imaginary place. There are many interpretations off what the metaverse would look like, and some people reject any corporation being a part of it. However, it has become the new buzzword that many companies are latching on to.\n",
      "Popular books, movies, and video games describe or create versions of the metaverse. Ernest Cline’s bestselling book Ready Player One that was later made into a movie by Steven Spielberg, paints a picture where people spend their time in a virtual reality world. This description is often used to describe the metaverse. The company Roblox (RBLX) is one of the first gaming companies to promote a metaverse. It provides a platform that allows users to create content based on games, entertainment, social media, and toys. Roblox has climbed nearly 70% from its IPO in March.\n",
      "Before Roblox, Activision Blizzard (NASDAQ:ATVI) was among the leading publicly traded companies for metaverse-type games, which includes World of Warcraft. However, there are many popular metaverse platforms like Fortnite, which is held privately by Epic Games, and Minecraft, which is owned by Microsoft.\n",
      "What Meta hopes to do is create its own virtual reality world where people can own, buy, and sell property and products. The company is investing $10 billion into the metaverse. But it doesn’t want to focus on just games. It hopes to build an entire social system.\n",
      "If this feels a little “out there” and too conceptual, it’s probably because it is. Meta CEO Mark Zuckerberg told investors that you have to experience the metaverse to know what it is. But it isn’t built so you can’t experience it. Investors and even gamers appear to be mixed on the future success of the metaverse. However, it’s a world that Meta is staking much of its future on.\n",
      "Michigan Consumer Sentiment Monthly Chart.\n",
      "CHART OF THE DAY: FEELING IT. Sharp (OTC:SHCAY) declines in the Michigan Consumer Sentiment (blue) often precedes recessions (gray), but not every decline results in a recession. FRED® is a registered trademark of the Federal Reserve Bank of St. Louis. The Federal Reserve Bank of St. Louis does not sponsor or endorse and is not affiliated with TD Ameritrade. Data Sources: ICE (NYSE:ICE), S&P Dow Jones Indices. Chart source: The thinkorswim® platform. For illustrative purposes only. Past performance does not guarantee future results.\n",
      "University of Michigan: Whether it’s the metaverse or the economy, sentiment can be an important indicator. Last week’s University of Michigan Consumer Sentiment report dropped to 2011 levels. In the past, falling consumer sentiment has preceded or coincided with recessions, which is why many investors use it as an indicator for the economy and the overall market. However, it isn’t an effective indicator when used by itself because there are many times consumer sentiment has fallen but the economy kept growing. It may be helpful to use other indicators for timing purposes—if you’re into that.\n",
      "Center of the Universe: If the metaverse is going to happen, it’s going to need a lot of computing power and that means semiconductors. This is why Meta announced a partnership with Advanced Micro Devices (NASDAQ:AMD) earlier this month to create a new high-performance and power-efficient processor called EPYC. But there are a lot of companies at the center of the metaverse like semiconductor maker Nvidia (NASDAQ:NVDA), which provides processing for servers, databases, cryptocurrency processing, and so forth. However, Nvidia announced earlier this month that its Omniverse platform will help users connect and create in 3D worlds.\n",
      "Cloudflare (NYSE:NET), which announced better-than-expected earnings earlier this month, is a web performance and security company that provides content delivery. Basically, it creates a freeway down the backbone of the internet. According to the company’s website, it provides internet requests for about 19% of Fortune 1,000 companies and processes 28 million HTTP requests per second.\n",
      "Of course, metaverse explorers need their gear, and Meta is already a player in this space with its Oculus virtual reality headset. Sony (NYSE:SONY), Microsoft, Alphabet (NASDAQ:GOOGL), Apple (NASDAQ:AAPL), and Qualcomm also create virtual reality (VR) headsets and devices. In fact, VR has been around long enough that many analysts wonder why it hasn’t seen greater adoption. The lack of VR interest could be a wrench in the metaverse cogs.\n",
      "Masters of the Metaverse: In order to create a space in the metaverse, many users will use Unity Software (NYSE:U). Unity announced better-than-expected earnings last week. It helps create 3D models of objects and buildings. It provides cloud engines for users creating their own avatars and spaces. Along with Unity, Matterport (NASDAQ:MTTR) also provides 3D modeling, photography, and digitization. It reported better-than-expected earnings but missed on revenue at the beginning of the month.\n",
      "Showing that the metaverse isn’t just about games, Microsoft announced plans to integrate its Teams messaging platform into its metaverse and will allow users to create 3D avatars.\n",
      "Whether the metaverse ends up being the next version of the internet or just a place for techies to hang out is yet to be seen. As with any new product, there will be early adopters and excitement, but there will also be winners and losers. So, explore the metaverse with caution. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Profit Margins Make Difference On Earnings From Lowe’s, Target\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Add a Comment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Related Articles\n",
      "\n",
      "\n",
      " \n",
      "Is The NASDAQ Composite Running On Empty?\n",
      "By Michael Kramer - Nov 26, 2021\n",
      "6\n",
      " This article was written exclusively for Investing.com.The stock market has seen a change in the tides. It is now pricing in the possibility of the tapering process ending sooner... \n",
      "\n",
      "\n",
      " \n",
      "A Muted Bank Holiday Session\n",
      "By Craig Erlam - Nov 25, 2021 It’s been an unsurprisingly muted day in the markets as the U.S. celebrates Thanksgiving and the rest of us are left to watch most asset classes tread water for most of the... \n",
      "\n",
      "\n",
      " \n",
      "Historical Trends Suggest A Strengthening Bullish Trend In December\n",
      "By Chris Vermeulen - Nov 26, 2021\n",
      "1\n",
      " I received many messages and emails asking my opinions related to the recent market volatility and sideways trending in the U.S. markets. Many traders see the recent downward price... \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Profit Margins Make Difference On Earnings From Lowe’s, Target\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Add a Comment\n",
      "\n",
      "\n",
      "Comment Guidelines \n",
      "\n",
      "\n",
      "We encourage you to use comments to engage with other users, share your perspective and ask questions of authors and each other. However, in order to maintain the high level of discourse we’ve all come to value and expect, please keep the following criteria in mind:  \n",
      "\n",
      "\n",
      "           Enrich the conversation, don’t trash it.\n",
      "\n",
      "\n",
      "          Stay focused and on track. Only post material that’s relevant to the topic being discussed. \n",
      "\n",
      "\n",
      "          Be respectful. Even negative opinions can be framed positively and diplomatically. Avoid profanity, slander or personal attacks directed at an author or another user. Racism, sexism and other forms of discrimination will not be tolerated.\n",
      "\n",
      "Use standard writing style. Include punctuation and upper and lower cases. Comments that are written in all caps and contain excessive use of symbols will be removed.\n",
      "NOTE: Spam and/or promotional messages and comments containing links will be removed. Phone numbers, email addresses, links to personal or business websites, Skype/Telegram/WhatsApp etc. addresses (including links to groups) will also be removed; self-promotional material or business-related solicitations or PR (ie, contact me for signals/advice etc.), and/or any other comment that contains personal contact specifcs or advertising will be removed as well. In addition, any of the above-mentioned violations may result in suspension of your account.\n",
      "Doxxing. We do not allow any sharing of private or personal contact or other information about any individual or organization. This will result in immediate suspension of the commentor and his or her account.\n",
      "Don’t monopolize the conversation. We appreciate passion and conviction, but we also strongly believe in giving everyone a chance to air their point of view. Therefore, in addition to civil interaction, we expect commenters to offer their opinions succinctly and thoughtfully, but not so repeatedly that others are annoyed or offended. If we receive complaints about individuals who take over a thread or forum, we reserve the right to ban them from the site, without recourse.\n",
      "Only English comments will be allowed.\n",
      "\n",
      "Perpetrators of spam or abuse will be deleted from the site and prohibited from future registration at Investing.com’s discretion. \n",
      "I have read Investing.com's comments guidelines and agree to the terms described.\n",
      "\n",
      "I Agree \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Write your thoughts here\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Are you sure you want to delete this chart?\n",
      "\n",
      "Delete Cancel\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Post\n",
      "\n",
      "Post also to:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Replace the attached chart with a new chart ?\n",
      "\n",
      "Replace Cancel\n",
      "\n",
      "\n",
      "\n",
      "1000\n",
      "\n",
      "Comment Guidelines\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ability to comment is currently suspended due to negative user reports. Your status will be reviewed by our moderators.\n",
      "\n",
      "\n",
      "\n",
      "Please wait a minute before you try to comment again.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thanks for your comment. Please note that all comments are pending until approved by our moderators. It may therefore take some time before it appears on our website.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\n",
      "\t\t\t\t\n",
      "    \n",
      "       \n",
      "        \n",
      "                \t\t\t\t\t\n",
      "                    \t{username}\n",
      "\t\t\t\t\t\n",
      "                                Just Now\n",
      "                \n",
      "\t\n",
      "\t\n",
      "\t\t\n",
      "\t\t\tShare\n",
      "\t\t\n",
      "\n",
      "        \n",
      "            \n",
      "                Follow this post            \n",
      "            \n",
      "                Unfollow this post            \n",
      "        \n",
      "\n",
      "\t\t\t\t\n",
      "\t\t\n",
      "\t\t\t\n",
      "\t\t\tSave\t\t\n",
      "\t\n",
      "\t\n",
      "\t\t\n",
      "\n",
      "\tSaved. See Saved Items.\n",
      "\t\n",
      "\n",
      "\n",
      "\tThis comment has already been saved in your Saved Items\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\n",
      "\t\t\t\n",
      "\t\t\n",
      "\t\n",
      "\n",
      "                 Author's response             \n",
      "\t\t\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\n",
      "\t\t\t\t\t\t{commentContent}\n",
      "\t\t\t\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\tReply\n",
      "\t\t\t\t\t\n",
      "\t\n",
      "\t\n",
      "\t0\n",
      "\t\n",
      "\t\n",
      "\t0\n",
      "\n",
      "\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\tReport\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t   \n",
      "\t\n",
      "\t\t\t\t\n",
      "\t\t\t\n",
      "\t\t\t\t\n",
      "    \n",
      "\t\t\n",
      "            \t\t\t\t\n",
      "\t\t\t\t\t{username}\n",
      "\t\t\t\t\n",
      "                        Just Now\n",
      "\t\t\t Author's response \t\t\t\n",
      "\t\n",
      "\t\n",
      "\t\t\n",
      "\t\t\tShare\n",
      "\t\t\n",
      "\n",
      "        \n",
      "            \n",
      "                Follow this post            \n",
      "            \n",
      "                Unfollow this post            \n",
      "        \n",
      "\n",
      "\t\t\t\t\n",
      "\t\t\n",
      "\t\t\t\n",
      "\t\t\tSave\t\t\n",
      "\t\n",
      "\t\n",
      "\t\t\n",
      "\n",
      "\tSaved. See Saved Items.\n",
      "\t\n",
      "\n",
      "\n",
      "\tThis comment has already been saved in your Saved Items\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\n",
      "\t\t\t\n",
      "\t\t\n",
      "\t\n",
      "\n",
      "\n",
      "        \n",
      "\t\t\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\n",
      "\t\t\t\t\n",
      "                    \n",
      "                    \n",
      "                    \n",
      "                \n",
      "\t\t\t\t\t\t{commentContent}\n",
      "\t\t\t\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\tReply\n",
      "\t\t\t\t\t\n",
      "\t\n",
      "\t\n",
      "\t0\n",
      "\t\n",
      "\t\n",
      "\t0\n",
      "\n",
      "\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\tReport\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\n",
      "    \n",
      "\t\t\t\n",
      "\t\t\t\n",
      "\t\t\t\t\n",
      "\t\t\t\t\tShow more comments ()\n",
      "\t\t\t\t\n",
      "\t\t\t\n",
      "\t\t\t\n",
      "\t\t\t\t\n",
      "\t\t\t\t\tShow more replies ()\n",
      "\t\t\t\t\n",
      "\t\t\t\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Are you sure you want to delete this chart?\n",
      "\n",
      "Delete Cancel\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Post\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Replace the attached chart with a new chart ?\n",
      "\n",
      "Replace Cancel\n",
      "\n",
      "\n",
      "\n",
      "1000\n",
      "\n",
      "Comment Guidelines\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ability to comment is currently suspended due to negative user reports. Your status will be reviewed by our moderators.\n",
      "\n",
      "\n",
      "\n",
      "Please wait a minute before you try to comment again.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Add Chart to Comment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cancel\n",
      "Attach \n",
      "\n",
      "\n",
      "Confirm Block\n",
      "\n",
      "Are you sure you want to block %USER_NAME%?\n",
      "By doing so, you and %USER_NAME% will not be able to see\n",
      "any of each other's Investing.com's posts.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "%USER_NAME% was successfully added to your Block List\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Since you’ve just unblocked this person, you must wait 48 hours before renewing the block.\n",
      "\n",
      "\n",
      "\n",
      "Confirm Block Cancel\n",
      "\n",
      "\n",
      "Ok \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report this comment\n",
      "\n",
      "I feel that this comment is:\n",
      "Spam\n",
      "Offensive\n",
      "\n",
      "\n",
      "Submit \n",
      "\n",
      "\n",
      "Comment flagged\n",
      "\n",
      "\n",
      "\n",
      "Thank You!\n",
      "Your report has been sent to our moderators for review \n",
      "\n",
      "\n",
      "Close \n",
      " \n",
      "\n",
      "Disclaimer: Fusion Media would like to remind you that the data contained in this website is not necessarily real-time nor accurate. All CFDs (stocks, indexes, futures) and Forex prices are not provided by exchanges but rather by market makers, and so prices may not be accurate and may differ from the actual market price, meaning prices are indicative and not appropriate for trading purposes. Therefore Fusion Media doesn`t bear any responsibility for any trading losses you might incur as a result of using this data.\n",
      "Fusion Media or anyone involved with Fusion Media will not accept any liability for loss or damage as a result of reliance on the information including data, quotes, charts and buy/sell signals contained within this website. Please be fully informed regarding the risks and costs associated with trading the financial markets, it is one of the riskiest investment forms possible. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Indices\n",
      "Commodities\n",
      "ETFs\n",
      "Stocks\n",
      "\n",
      "More Categories\n",
      "\n",
      "Tabs SelectionIndicesCommoditiesETFsStocksCryptoForexBonds%COUNT%/4 selected Apply1D1W1M6M1Y5YMax US 3034,715.5-1120.5-3.13%  US 5004,582.6-126.3-2.68%  Dow Jones34,899.34-905.04-2.53%  S&P 5004,603.20-98.26-2.09%  Nasdaq15,491.7-353.6-2.23%  S&P 500 VIX28.62+10.04+54.04%  Dollar Index96.078-0.794-0.82%  Crude Oil WTI68.17-10.22-13.04%  Brent Oil72.69-9.53-11.59%  Natural Gas5.498+0.430+8.48%  Gold1,791.40+7.10+0.40%  Silver23.145-0.351-1.49%  Copper4.2790-0.1585-3.57%  US Soybeans1,253.38-18.62-1.46%  SPDR S&P 500458.97-10.47-2.23%  iShares Russell 2000222.95-8.63-3.73%  Invesco QQQ Trust390.99-7.77-1.95%  Direxion Daily Junior Gold Miners Bull 2X Shares65.38-3.87-5.59%  iShares Silver21.39-0.41-1.86%  ProShares Ultra VIX Short-Term Futures21.39+5.98+38.81%  ProShares UltraPro QQQ160.27-9.96-5.85%  Apple156.98-4.96-3.06%  Alphabet A2,852.8-69.6-2.38%  Tesla1,082.06-33.94-3.04%  Amazon.com3,507.76-72.65-2.03%  Netflix665.81+7.52+1.14%  Meta Platforms333.20-7.86-2.30%  Bank of America45.77-1.86-3.91%  \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " Most Popular Articles\n",
      "\n",
      "Analysis\n",
      "News\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Risk assets plunge as virus fears cause post-Thanksgiving blues\n",
      "By Reuters - Nov 26, 2021\n",
      "143\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "Oil plunges $10/bbl on new coronavirus variant concerns\n",
      "By Reuters - Nov 26, 2021\n",
      "15\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "Wall Street Opens Sharply Lower as Covid Concerns Flare Again; Dow...\n",
      "By Investing.com - Nov 26, 2021\n",
      "49\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "New Covid-19 Strain, Markets Tumble, Black Friday - What's Moving...\n",
      "By Investing.com - Nov 26, 2021\n",
      "46\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "Dow Futures Down 855 Pts as New Covid Strain Overshadows Black Friday\n",
      "By Investing.com - Nov 26, 2021\n",
      "45\n",
      " \n",
      "\n",
      " More News \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Zoom Stock’s 38% Plunge Makes It An Attractive Buy\n",
      "By Haris Anwar/Investing.com - Nov 26, 2021\n",
      "6\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "Is The NASDAQ Composite Running On Empty?\n",
      "By Michael Kramer - Nov 26, 2021\n",
      "6\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "3 Stocks Poised For New Highs As Fed Rate Hikes Expected Sooner Than...\n",
      "By Jesse Cohen/Investing.com - Nov 24, 2021\n",
      "6\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "You Can Have 10.2% Dividends, 250% Gains In 1 Buy (Here’s The Ticker)\n",
      "By Michael Foster - Nov 22, 2021\n",
      "1\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "Decred: A Top Tier Cryptocurrency\n",
      "By Andy Hecht - Nov 25, 2021\n",
      "7\n",
      " \n",
      "\n",
      " More Analysis \n",
      "\n",
      "\n",
      " \n",
      "Central Banks Rates\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Central Banks\n",
      "Interest Rates\n",
      "Next Meeting\n",
      " \n",
      "\n",
      "\n",
      "  FED0.00%-0.25%Dec 15, 2021   ECB0.00%Dec 16, 2021   BOE0.10%Dec 16, 2021   SNB-0.75%Dec 16, 2021   RBA0.10%Dec 07, 2021   BOC0.25%Dec 08, 2021   RBNZ0.75%Nov 24, 2021   BOJ-0.10%Dec 17, 2021   CBR7.50%Dec 17, 2021   RBI4.00%Dec 08, 2021   PBOC3.85%   BCB7.75%Dec 08, 2021 \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Trade With A Regulated Broker\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Investing.com\n",
      "\n",
      "Blog\n",
      "Mobile\n",
      "Portfolio\n",
      "Widgets\n",
      "\n",
      "\n",
      "About Us\n",
      "Advertise\n",
      "Help & Support\n",
      "Authors\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Our Apps\n",
      " \n",
      "\n",
      "Follow us\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Terms And Conditions\n",
      "Privacy Policy\n",
      "Risk Warning\n",
      "\n",
      "© 2007-2021 Fusion Media Limited. All Rights Reserved \n",
      "\n",
      "Risk Disclosure: Trading in financial instruments and/or cryptocurrencies involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors. Prices of cryptocurrencies are extremely volatile and may be affected by external factors such as financial, regulatory or political events. Trading on margin increases the financial risks.\n",
      "Before deciding to trade in financial instrument or cryptocurrencies you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.\n",
      "Fusion Media would like to remind you that the data contained in this website is not necessarily real-time nor accurate. The data and prices on the website are not necessarily provided by any market or exchange, but may be provided by market makers, and so prices may not be accurate and may differ from the actual price at any given market, meaning prices are indicative and not appropriate for trading purposes. Fusion Media and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information contained within this website.\n",
      "It is prohibited to use, store, reproduce, display, modify, transmit or distribute the data contained in this website without the explicit prior written permission of Fusion Media and/or the data provider. All intellectual property rights are reserved by the providers and/or the exchange providing the data contained in this website.\n",
      "Fusion Media may be compensated by the advertisers that appear on the website, based on your interaction with the advertisements or advertisers. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Sign up for FREE and get:\n",
      " Real-Time Alerts\n",
      " Advanced Portfolio Features\n",
      " Personalized Charts\n",
      " Fully-Synced App\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Continue with Facebook\n",
      "\n",
      "\n",
      "\n",
      "Continue with Google\n",
      "\n",
      "\n",
      "or\n",
      "\n",
      "Sign up with Email \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus.documents[np.argmax(testscr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03278833 0.03464162 0.03663881 0.03534797 0.03742775 0.03670947\n",
      " 0.03681745 0.03865908 0.01791634 0.03733829 0.03406842 0.03842224\n",
      " 0.         0.03491197 0.         0.03931505 0.03926478 0.03947661\n",
      " 0.03767367 0.01319947 0.03876723 0.03634822 0.03548198 0.03790575\n",
      " 0.03766551 0.03524931 0.03945156 0.02165579 0.03858625 0.03844825\n",
      " 0.03199028 0.         0.02547845 0.02898359 0.02895752 0.03281793\n",
      " 0.02794258 0.02745618 0.02082897 0.01799648 0.02118542 0.03889935\n",
      " 0.03369837 0.02327666 0.02137535 0.02184687 0.02038056 0.02679584\n",
      " 0.03116039 0.02246439 0.03812491 0.02195865 0.03490528 0.03868128\n",
      " 0.01443033 0.03697392 0.02854032 0.03831119 0.03856004 0.02670609\n",
      " 0.02253517 0.02239406 0.03772622 0.03157525 0.02674308 0.03757054\n",
      " 0.03804713 0.01859964 0.0156894  0.03516166 0.03010901 0.02159042\n",
      " 0.03566706 0.0288699  0.03848151 0.03404253 0.03366276 0.03874201\n",
      " 0.0216485  0.03557348 0.03374597 0.0390306  0.03345284 0.03566923\n",
      " 0.02013829 0.03709963 0.02810668 0.03079518 0.02354914 0.03812674\n",
      " 0.02255096 0.01448879 0.02227781 0.03896619 0.02764657 0.015938\n",
      " 0.03838745 0.03888593 0.02868857 0.03867235 0.02884928 0.02189144\n",
      " 0.03487096 0.0217069  0.02052351 0.0182487  0.01863194 0.0381132\n",
      " 0.01742178 0.02183205 0.03459043 0.02229324 0.01907607 0.0206884\n",
      " 0.03956619 0.03726605 0.03879537 0.02103314 0.01537464 0.02305987\n",
      " 0.01535997 0.03211763 0.03112941 0.02398846 0.03800562 0.02292846\n",
      " 0.03076193 0.02785392 0.03630917 0.03722654 0.02566958 0.01745289\n",
      " 0.02339917 0.02733354 0.01735602 0.02305987 0.02237071 0.03008678\n",
      " 0.03436882 0.01788369 0.02127644 0.01745219 0.02250366 0.0227824\n",
      " 0.02143228 0.02180986 0.02263023 0.02058915 0.02058257 0.02592457\n",
      " 0.02186171 0.01541145 0.03172197 0.0369979  0.03910602 0.02796505\n",
      " 0.02154571 0.0271491  0.02647181 0.03773868 0.02271808 0.02552107\n",
      " 0.03122084 0.02345476 0.01826424 0.02299398 0.02406738 0.02296117\n",
      " 0.03779663 0.02066556 0.03827906 0.03348765 0.02137535 0.01787654\n",
      " 0.03861711 0.01788648 0.02374019 0.01486346 0.0266492  0.0394435\n",
      " 0.01960472 0.02186171 0.01193532 0.03034867 0.02254306 0.01723262\n",
      " 0.02159042 0.0370509  0.03856254 0.02141802 0.02252728]\n",
      "114\n",
      "(array([12, 14, 31], dtype=int64),)\n",
      "0.028008147634918927\n",
      "0.01745219181057231\n"
     ]
    }
   ],
   "source": [
    "testsc=bm25.score(testq, corpus.documents)\n",
    "print(testsc)\n",
    "print(np.argmax(testsc))\n",
    "print(np.where(testsc==0))\n",
    "print(np.mean(testsc))\n",
    "print(testsc[141])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Apple Inc.', 20], ['Technology', 0.01], ['Consumer Electronics', 0.01], ['investing', 0.01], ['analysis', 0.1]]\n"
     ]
    }
   ],
   "source": [
    "testqc = [[tick.name, 20]]\n",
    "for q in tick.tags:\n",
    "    testqc.append([q, 0.01])\n",
    "testqc.append(['investing',0.01])\n",
    "testqc.append(['analysis',0.1])\n",
    "print(testqc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0c4facd32301>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbm25q\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_expanded\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestqc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\UIUC\\CS_410\\Project\\Repo\\CourseProject\\util\\pyRanker2.py\u001b[0m in \u001b[0;36mscore_expanded\u001b[1;34m(self, Q, X, weighted_tf)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# apply CountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mdl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mqw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'super' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "bm25q.score_expanded(testqc, corpus.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-docs: 158\n"
     ]
    }
   ],
   "source": [
    "#create the sub_documents, wrapper to run multiple functions\n",
    "#passing in the tokenizer to save a little on class dependencies\n",
    "corpus.sub_divide(tokenizer=tokenizer, cutoff=2, method='sen')\n",
    "print('Sub-docs:',len(corpus.sub_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass in ticker object and use the BM25 ranker to do a collection of ranking\n",
    "#this is the same as running each commented function below one by one\n",
    "\n",
    "corpus.rank_ticker(tick,bm25)\n",
    "\n",
    "# corpus.rank_docs(tick.name, bm25)\n",
    "# corpus.rank_doc_tags(tick.tags, bm25)\n",
    "# corpus.rank_subdocs(tick.name, bm25)\n",
    "# corpus.rank_subdocs_tags(tick.tags, bm25)\n",
    "# corpus.rank_titles(tick.name,bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 'relevant set' by pruning the sub_doc based on a cutoff value for the ranker score\n",
    "corpus.prune_subdocs(cutoff=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_set = corpus.relevant_set\n",
    "relevant_scores = corpus.relevant_scores\n",
    "print(len(relevant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92653d83bd33458581cb5283a359b122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=143)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run the new relevant set through distilled-BERT and get sentiment classifications\n",
    "sentiments = get_sentiments(relevant_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens=[]\n",
    "for x in relevant_set.keys():\n",
    "    for y in range(0, len(relevant_set[x])):\n",
    "        lens.append(len(relevant_set[x][y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "avlen=np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_weight={}\n",
    "for x in relevant_set.keys():\n",
    "    l=[]\n",
    "    for y in range(0, len(relevant_set[x])):\n",
    "        l.append(len(relevant_set[x][y])/avlen)\n",
    "    len_weight[x]=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rel = {}\n",
    "for x in relevant_scores.keys():\n",
    "    l=[]\n",
    "    for y in range(0, len(relevant_scores[x])):\n",
    "        l.append(relevant_scores[x][y] * len_weight[x][y])\n",
    "    adjusted_rel[x]=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for x in relevant_set.keys():\n",
    "#     print(x, \"relevance:\", relevant_scores[x])\n",
    "#     print(\"  adjusted r:\", adjusted_rel[x])\n",
    "#     print(\"  sentiments:\", sentiments[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_scores=[]\n",
    "lrw_scores=[]\n",
    "for x in relevant_scores.keys():\n",
    "    for y in range(0, len(relevant_scores[x])):\n",
    "        rw_scores.append(relevant_scores[x][y] * sentiments[x][y])\n",
    "        lrw_scores.append(adjusted_rel[x][y] * sentiments[x][y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Relevance weighted Sentiment: -0.2161\n",
      "Average Length adjusted Relevance weighted sentiment: -0.2239\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Relevance weighted Sentiment:\", np.mean(rw_scores).round(4))\n",
    "print(\"Average Length adjusted Relevance weighted sentiment:\", np.mean(lrw_scores).round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
