{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_metric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification\n",
    "from transformers import pipeline, Trainer\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "classifier = pipeline(task='sentiment-analysis',model=model,tokenizer=tokenizer)\n",
    "max_tokens = int(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset financial_phrasebank (C:\\Users\\Tony\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_66agree\\1.0.0\\a6d468761d4e0c8ae215c77367e1092bead39deb08fbf4bffd7c0a6991febbf0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d94a829f77740f1b316bc394bff9e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Tony\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_66agree\\1.0.0\\a6d468761d4e0c8ae215c77367e1092bead39deb08fbf4bffd7c0a6991febbf0\\cache-4e0666b22dba4755.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Tony\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_66agree\\1.0.0\\a6d468761d4e0c8ae215c77367e1092bead39deb08fbf4bffd7c0a6991febbf0\\cache-a1ade6443fd22d2a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Tony\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_66agree\\1.0.0\\a6d468761d4e0c8ae215c77367e1092bead39deb08fbf4bffd7c0a6991febbf0\\cache-49c96cd7749fcc96.arrow\n",
      "Loading cached split indices for dataset at C:\\Users\\Tony\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_66agree\\1.0.0\\a6d468761d4e0c8ae215c77367e1092bead39deb08fbf4bffd7c0a6991febbf0\\cache-edbe7759452b8a26.arrow and C:\\Users\\Tony\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_66agree\\1.0.0\\a6d468761d4e0c8ae215c77367e1092bead39deb08fbf4bffd7c0a6991febbf0\\cache-96c5c0f62d9b694d.arrow\n"
     ]
    }
   ],
   "source": [
    "#LOADING IN CUSTOM DATASET TO FINE TUNE ACCURACY OF MODEL\n",
    "#needs verification whether it fits/work with the current model as the labels for the 2 models (pos, neg) are different\n",
    "#\n",
    "dataset = load_dataset(\n",
    "   'financial_phrasebank', 'sentences_66agree')\n",
    "\n",
    "#modifying dataset to fit format of pre-trained model\n",
    "custom_data = dataset['train']\n",
    "custom_data = custom_data.filter(lambda example: example['label'] % 2 == 0)\n",
    "\n",
    "#more modification\n",
    "def add_prefix(example):\n",
    "    if example['label'] == 2:\n",
    "        example['label'] = example['label'] - 1\n",
    "    return example\n",
    "\n",
    "#mapping in changes to dataset - 0 is negative, 1 is positive\n",
    "custom_data = custom_data.map(add_prefix)\n",
    "#print(custom_data['label'])\n",
    "        \n",
    "#tokenizer for processing data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=True, truncation=True)\n",
    "\n",
    "#mapping and splitting up custom data into training and testing\n",
    "tokenized_datasets = custom_data.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)\n",
    "\n",
    "valid_set = DatasetDict({\n",
    "    'train': tokenized_datasets['train'],\n",
    "    'test': tokenized_datasets['test']})\n",
    "#print(valid_set)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#error handling\n",
    "columns_to_return = ['input_ids', 'label', 'attention_mask']\n",
    "valid_set.set_format(type='torch', columns=columns_to_return)\n",
    "\n",
    "#print(valid_set)\n",
    "#train model with new data and report accuracy\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    pred = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence.\n",
      "***** Running training *****\n",
      "  Num examples = 1513\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='760' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [760/760 29:29, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.164709</td>\n",
       "      <td>0.970414</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974576</td>\n",
       "      <td>0.978723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 169\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./output\\checkpoint-500\n",
      "Configuration saved in ./output\\checkpoint-500\\config.json\n",
      "Model weights saved in ./output\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./output\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./output\\checkpoint-500\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output\\checkpoint-500 (score: 0.16470938920974731).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=760, training_loss=0.03764423972093745, metrics={'train_runtime': 1771.7222, 'train_samples_per_second': 6.832, 'train_steps_per_second': 0.429, 'total_flos': 250528967708160.0, 'train_loss': 0.03764423972093745, 'epoch': 8.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fine tuning the model with custom dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=valid_set['train'],\n",
    "    eval_dataset=valid_set['test'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#check in future to see if this is feasible and makes sense with the current pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./model/\n",
      "Configuration saved in ./model/config.json\n",
      "Model weights saved in ./model/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/tokenizer_config.json\n",
      "Special tokens file saved in ./model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#this saves locally, the models can be very large. It is recommended that you push them to huggingface's model hub\n",
    "trainer.save_model('./model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
