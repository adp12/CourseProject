{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dateutil import parser as dparser\n",
    "import urllib.parse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy import sparse\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from util.config import config\n",
    "from util.pyBM25 import BM25\n",
    "from util.web_query import web_query\n",
    "from util.ticker import Ticker\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "classifier = pipeline(task='sentiment-analysis',model=model,tokenizer=tokenizer)\n",
    "max_tokens = int(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running through sub_docs and outputting sentiment list\n",
    "\n",
    "def get_sentiments(docs):\n",
    "    \n",
    "    c = 0\n",
    "    for x in docs.keys():\n",
    "        c+=len(docs[x])\n",
    "    \n",
    "    pgres = widgets.IntProgress(value=0,min=0,max=c, step=1)\n",
    "    display(pgres)\n",
    "    \n",
    "    sentiments = {}\n",
    "    \n",
    "    for x in docs.keys():\n",
    "        scrs=[]\n",
    "        for y in range(0, len(docs[x])):\n",
    "            \n",
    "            s = classifier(docs[x][y])\n",
    "            scr = s[0]['score']\n",
    "            if s[0]['label']==\"NEGATIVE\":\n",
    "                scr=scr*-1\n",
    "            scrs.append(scr)\n",
    "            pgres.value+=1\n",
    "            pgres.description=str(pgres.value)+\":\"+str(c)\n",
    "        \n",
    "        sentiments[x]=scrs\n",
    "                \n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the rows of a 2d input_matrix so they sum to 1\n",
    "    \"\"\"\n",
    "\n",
    "    row_sums = input_matrix.sum(axis=1)\n",
    "    try:\n",
    "        assert (np.count_nonzero(row_sums)==np.shape(row_sums)[0]) # no row should sum to zero\n",
    "    except Exception:\n",
    "        raise Exception(\"Error while normalizing. Row(s) sum to zero\")\n",
    "    new_matrix = input_matrix / row_sums[:, np.newaxis]\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    #******************************************************************************\n",
    "    #----------------------------------Method---------------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    #Run the web_query to produce a collection of text documents scraped from the web\n",
    "    \n",
    "    #Use the set_results() function to store the full results in the corpus for processing\n",
    "    \n",
    "    #Use the set_corpus() function to assign the documents scraped from the web to the corpus\n",
    "    \n",
    "    #Sub divide the documents into smaller sub_docs\n",
    "    \n",
    "    #Rank the documents based on relevance to the original query as well as any tags\n",
    "    \n",
    "    #Prune the sub_docs to produce a relevant set\n",
    "    \n",
    "    #\n",
    "    \n",
    "    #******************************************************************************\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #typical corpus data\n",
    "        self.documents = []\n",
    "        self.vocabulary = []\n",
    "        self.number_of_documents = 0\n",
    "        self.vocabulary_size = 0\n",
    "        \n",
    "        #plsa and liklihoods\n",
    "        self.likelihoods = []\n",
    "        self.term_doc_matrix = None \n",
    "        self.document_topic_prob = None  # P(z | d)\n",
    "        self.topic_word_prob = None  # P(w | z)\n",
    "        self.topic_prob = None  # P(z | d, w)\n",
    "                \n",
    "        #for web results\n",
    "        self.query_results=None\n",
    "        self.max_tokens=512\n",
    "        self.failed = []\n",
    "        \n",
    "        #sub dividing documents\n",
    "        self.tokenizer=None\n",
    "        self.sub_docs=None\n",
    "        \n",
    "        #relevance scores\n",
    "        self.document_scores=None\n",
    "        self.document_tag_scores=None\n",
    "        self.subdoc_scores=None\n",
    "        self.subdoc_tag_scores=None\n",
    "        self.title_scores=None\n",
    "                \n",
    "        #pruned data\n",
    "        self.relevant_set=None\n",
    "        self.relevant_scores=None\n",
    "    \n",
    "    \n",
    "    #******************************************************************************\n",
    "    #------------------------------Setting Corpus----------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def set_results(self, df):\n",
    "        #dataframe returned from webquery\n",
    "        self.query_results=df\n",
    "    \n",
    "    def set_corpus(self, documents):\n",
    "        self.documents = documents\n",
    "        \n",
    "    def build_corpus_from_url(self, max_docs=50):\n",
    "        \n",
    "        #scrape text from url-list to build corpus\n",
    "        #(not recommended, use the same method from the web_query object and the set_corpus() method)\n",
    "        \n",
    "        url_list = self.query_results['url'].tolist()\n",
    "        url_list = url_list[0:max_docs]\n",
    "        \n",
    "        pgres = widgets.IntProgress(value=0,min=0,max=len(url_list), step=1)\n",
    "        display(pgres)\n",
    "        \n",
    "        failed=[]\n",
    "        headers = {\"User-Agent\":\"Mozilla/5.0\"}\n",
    "        for i in range(0,len(url_list)):\n",
    "            try:\n",
    "                response = requests.get(url=url_list[i],headers=headers)\n",
    "                if response.status_code==200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    d = soup.get_text()\n",
    "                    if len(d)>200:\n",
    "                        self.documents.append(d)\n",
    "                else:\n",
    "                    self.failed.append(i)\n",
    "            except:\n",
    "                self.failed.append(i)\n",
    "\n",
    "            finally:\n",
    "                pgres.value+=1\n",
    "                pgres.description=str(i+1)+\":\"+str(len(url_list))\n",
    "                \n",
    "        self.number_of_documents=len(self.documents)\n",
    "        #remove failed url responses from dataset\n",
    "        self.query_results = self.query_results.take(list(set(range(self.query_results.shape[0]))-set(self.failed)))\n",
    "        \n",
    "\n",
    "    def build_corpus_from_file(self, file_path):\n",
    "\n",
    "        f = open(file_path, 'r')\n",
    "        docs = f.readlines()\n",
    "        for d in docs:\n",
    "            self.documents.append(d)\n",
    "        self.number_of_documents = len(docs)\n",
    "\n",
    "        \n",
    "    def build_vocabulary(self, stopwords):\n",
    "\n",
    "        v = set([])\n",
    "        for x in self.documents:\n",
    "            tmp = set(x.split())\n",
    "            tmp = {x for x in tmp if x.lower() not in stopwords}\n",
    "                        \n",
    "            v.update(tmp)\n",
    "        \n",
    "        v = list(v)\n",
    "        self.vocabulary = v\n",
    "        self.vocabulary_size = len(v)\n",
    "        \n",
    "             \n",
    "    \n",
    "    #******************************************************************************\n",
    "    #------------------------------Sub Dividing-------------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def get_pgraphs(self, doc, cutoff, method):\n",
    "        #updated get_pgraphs() with method for cutoff\n",
    "        #cut off method:\n",
    "        #sen: number of sentences\n",
    "        #word: number of words  \n",
    "\n",
    "        pgraphs=[]\n",
    "        freshsoup = re.split('\\n\\n',doc)\n",
    "        for x in range(0,len(freshsoup)):\n",
    "            if method=='word':\n",
    "                words = len(str(freshsoup[x]).strip().split(' ',maxsplit=cutoff))\n",
    "                if words>cutoff:\n",
    "                    pgraphs.append(freshsoup[x])\n",
    "                    \n",
    "            elif method=='sen':\n",
    "                sens = len(re.findall(\"\\.\",str(freshsoup[x]).strip()))\n",
    "                if sens>cutoff:\n",
    "                    pgraphs.append(freshsoup[x])\n",
    "                    \n",
    "        return pgraphs\n",
    "    \n",
    "    def split_doc(self, doc, subs):         \n",
    "            \n",
    "        if len(re.findall(r'\\.', doc))>1:\n",
    "            cut_point = doc.rfind('.', 0, int(len(doc)/2))+1\n",
    "        else:\n",
    "            cut_point = int(len(doc)/2)\n",
    "\n",
    "        d1 = doc[0:cut_point]\n",
    "        d2 = doc[cut_point+1:]\n",
    "\n",
    "        tkns1 = int(len(self.tokenizer(d1)['input_ids']))\n",
    "\n",
    "        if tkns1>self.max_tokens:\n",
    "            self.split_doc(d1,subs)\n",
    "        else:\n",
    "            if len(d1)>0:\n",
    "                subs.append(d1)\n",
    "\n",
    "        tkns2 = int(len(self.tokenizer(d2)['input_ids']))\n",
    "\n",
    "        if tkns2>self.max_tokens:\n",
    "            self.split_doc(d2, subs)\n",
    "        else:\n",
    "            if len(d2)>0:\n",
    "                subs.append(d2)\n",
    "            \n",
    "    \n",
    "    def get_subdocs(self, pgraphs):\n",
    "        #Updated get_subdocs with iterative slicing \n",
    "        #ensure sub_docs tokens will not exceed max_tokens for sentiment model\n",
    "        sub_docs=[]\n",
    "\n",
    "        for x in range(0, len(pgraphs)):\n",
    "            sen_cnt = len(re.split('\\n|\\. ',pgraphs[x]))\n",
    "            tkns = int(len(tokenizer(pgraphs[x])['input_ids']))\n",
    "\n",
    "            if tkns<self.max_tokens:\n",
    "                sub_docs.append(pgraphs[x])\n",
    "            else:\n",
    "                self.split_doc(pgraphs[x],sub_docs)\n",
    "        \n",
    "        return sub_docs\n",
    "        \n",
    "    def sub_divide(self, tokenizer, cutoff=1, method='sen'):\n",
    "\n",
    "        #creates a dictionary of sub_docs divided from each document in the corpus\n",
    "        #method: using get_pgraphs() followed by get_subdocs()\n",
    "        #output form: dict{ document_id : [subdoc_1, subdoc_2 ... subdoc_n] }\n",
    "\n",
    "        subbed_data = {}\n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "        for x in range(0, len(self.documents)):\n",
    "\n",
    "            pg = self.get_pgraphs(self.documents[x], cutoff, method)\n",
    "            subs = self.get_subdocs(pg)\n",
    "            subbed_data[x]=subs\n",
    "\n",
    "        self.sub_docs = subbed_data\n",
    "\n",
    "        \n",
    "    #******************************************************************************\n",
    "    #----------------------------------Relevance Scoring---------------------------\n",
    "    #******************************************************************************  \n",
    "    \n",
    "    def rank_docs(self, query, ranker):\n",
    "        self.document_scores = ranker.score(query, self.documents)\n",
    "        \n",
    "    def rank_doc_tags(self, tags, ranker):\n",
    "        tag_scores=[]\n",
    "        for t in tags:\n",
    "            scores = ranker.score(t, self.documents)\n",
    "            tag_scores.append(scores)\n",
    "            \n",
    "        self.document_tag_scores = tag_scores\n",
    "        \n",
    "    def rank_subdocs(self, query, ranker):\n",
    "        sub_vecs={}\n",
    "        for x in self.sub_docs.keys():\n",
    "            sub_vec = ranker.score(query, self.sub_docs[x])\n",
    "            sub_vecs[x]=sub_vec\n",
    "            \n",
    "        self.subdoc_scores = sub_vecs\n",
    "    \n",
    "    def rank_subdocs_tags(self, tags, ranker):\n",
    "        \n",
    "        tag_scores=[]\n",
    "        for t in tags:\n",
    "            sub_vecs={}\n",
    "            for x in self.sub_docs.keys():\n",
    "                sub_vec = ranker.score(t, self.sub_docs[x])\n",
    "                sub_vecs[x]=sub_vec\n",
    "            tag_scores.append(sub_vecs)\n",
    "        \n",
    "        self.subdoc_tag_scores = tag_scores\n",
    "    \n",
    "    def rank_titles(self, name, ranker):\n",
    "        name = re.sub('(,|\\.|Inc| )',\"\",str(name))\n",
    "        titles = self.query_results['title'].tolist()\n",
    "        self.title_scores = ranker.score(name, titles)\n",
    "        \n",
    "    def rank_ticker(self, ticker, ranker):\n",
    "        \n",
    "        #Takes a ticker object and runs all of the rankers above\n",
    "        \n",
    "        name = ticker.name\n",
    "        sym = ticker.ticker\n",
    "        tags = ticker.tags\n",
    "        \n",
    "        self.rank_docs(name,ranker)\n",
    "        self.rank_doc_tags(tags, ranker)\n",
    "        self.rank_subdocs(name,ranker)\n",
    "        self.rank_subdocs_tags(tags,ranker)\n",
    "        self.rank_titles(name,ranker)\n",
    "        \n",
    "    #******************************************************************************\n",
    "    #----------------------------Pruning Relevant Set------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def prune_subdocs(self, cutoff=0.4):\n",
    "        subbed_data = self.sub_docs\n",
    "        sub_scores = self.subdoc_scores\n",
    "        for x in self.sub_docs.keys():\n",
    "\n",
    "            subbed_data[x] = [xv if c else None for c, xv in zip(sub_scores[x]>cutoff, subbed_data[x])]\n",
    "            subbed_data[x] = [y for y in subbed_data[x] if y!=None]\n",
    "            sub_scores[x] = [y for y in sub_scores[x] if y>cutoff]\n",
    "        \n",
    "        self.relevant_set = {k: v for k, v in subbed_data.items() if len(v) > 0}\n",
    "        self.relevant_scores={k: v for k, v in sub_scores.items() if len(v) > 0}\n",
    "    \n",
    "    #******************************************************************************\n",
    "    #-------------------------------------PLSA (from MP3)--------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def build_term_doc_matrix(self):\n",
    "        \n",
    "        m = []\n",
    "        line = []\n",
    "        for x in self.documents:\n",
    "            doc = list(x.split())\n",
    "            for itm in self.vocabulary:\n",
    "                line.append(x.count(itm))\n",
    "            m.append(line)\n",
    "            line = []\n",
    "        self.term_doc_matrix = np.array(m)\n",
    "        \n",
    "    def initialize_prob(self, number_of_topics):\n",
    "\n",
    "        self.document_topic_prob = np.random.random_sample((self.number_of_documents, number_of_topics))\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "\n",
    "        self.topic_word_prob = np.random.random_sample((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "\n",
    "            \n",
    "    def E_step(self):\n",
    "        \n",
    "        for x in range(0,self.term_doc_matrix.shape[0]):  #loop through documents\n",
    "            e = self.document_topic_prob[x].reshape(-1,1)*self.topic_word_prob\n",
    "            self.topic_prob[x] = normalize(e)\n",
    "           \n",
    "\n",
    "    def M_step(self, number_of_topics):\n",
    "        \n",
    "        pz = []\n",
    "        for x in range(0, self.term_doc_matrix.shape[0]):         \n",
    "            m = self.topic_prob[x]*self.term_doc_matrix[x].reshape(1,-1)\n",
    "            self.document_topic_prob[x] = np.sum(m,axis=1)\n",
    "            pz.append(m)\n",
    "\n",
    "        #update\n",
    "        \n",
    "        pz = np.array(pz)\n",
    "        self.topic_word_prob = np.sum(pz,axis=0)\n",
    "        \n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    " \n",
    "\n",
    "    def calculate_likelihood(self, number_of_topics):\n",
    "\n",
    "        l = np.log(np.prod(np.power(np.dot(self.document_topic_prob,self.topic_word_prob),self.term_doc_matrix),axis=1))\n",
    "        l = l[np.argmax(l)]\n",
    "        self.likelihoods.append(l)\n",
    "        \n",
    "\n",
    "    def plsa(self, number_of_topics, max_iter, epsilon):\n",
    "\n",
    "        self.build_term_doc_matrix()\n",
    "        self.topic_prob = np.zeros([self.number_of_documents, number_of_topics, self.vocabulary_size], dtype=np.float)\n",
    "        self.initialize_prob(number_of_topics)\n",
    "        current_likelihood = 0.0\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "            self.E_step()\n",
    "            self.M_step(number_of_topics)\n",
    "            \n",
    "            l = self.calculate_likelihood(number_of_topics)\n",
    "            \n",
    "            if current_likelihood==0 or current_likelihood==None or l>current_likelihood:\n",
    "                current_likelihood = l\n",
    "            else:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull api keys from the config file\n",
    "cfig=config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a ticker object \n",
    "tick = Ticker(cfig, \"LMT\",source='yahoo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq=web_query(cfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281ceab27535472099290a5068334287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=200)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testq=tick.name\n",
    "d_start=\"11/1/2021\"\n",
    "#query all of the news apis in web_query object\n",
    "wq.query_all(query=tick.name, ticker=tick.ticker, d_start=d_start)\n",
    "#compile results into a singular dataframe\n",
    "wq.compile_results()\n",
    "#scrap text from the results urls to form documents\n",
    "wq.scrape_results(threaded=True, max_docs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wq.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build corpus from web query results\n",
    "corpus=Corpus()\n",
    "#store the web query data frame in the corpus for referencing urls and titles\n",
    "corpus.set_results(df)\n",
    "#assign corpus documents as the web query documents\n",
    "corpus.set_corpus(wq.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:22320\n",
      "Number of documents:158\n"
     ]
    }
   ],
   "source": [
    "#pull in stop words and build corpus vocabulary \n",
    "stopwords=[]\n",
    "with open('util/stopwords.txt') as f:\n",
    "    stopwords.append(f.read().splitlines())\n",
    "stopwords=stopwords[0]\n",
    "\n",
    "corpus.build_vocabulary(stopwords)\n",
    "\n",
    "print(\"Vocabulary size:\" + str(len(corpus.vocabulary)))\n",
    "print(\"Number of documents:\" + str(len(corpus.documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build BM25 ranker fit to the corpus vocabulary\n",
    "bm25 = BM25(norm='l2', smooth_idf=True, stopwords=stopwords, sublinear_tf=True, vocabulary=corpus.vocabulary)\n",
    "bm25.fit(corpus.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-docs: 158\n"
     ]
    }
   ],
   "source": [
    "#create the sub_documents, wrapper to run multiple functions\n",
    "#passing in the tokenizer to save a little on class dependencies\n",
    "corpus.sub_divide(tokenizer=tokenizer, cutoff=2, method='sen')\n",
    "print('Sub-docs:',len(corpus.sub_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass in ticker object and use the BM25 ranker to do a collection of ranking\n",
    "#this is the same as running each commented function below one by one\n",
    "\n",
    "corpus.rank_ticker(tick,bm25)\n",
    "\n",
    "# corpus.rank_docs(tick.name, bm25)\n",
    "# corpus.rank_doc_tags(tick.tags, bm25)\n",
    "# corpus.rank_subdocs(tick.name, bm25)\n",
    "# corpus.rank_subdocs_tags(tick.tags, bm25)\n",
    "# corpus.rank_titles(tick.name,bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 'relevant set' by pruning the sub_doc based on a cutoff value for the ranker score\n",
    "corpus.prune_subdocs(cutoff=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_set = corpus.relevant_set\n",
    "relevant_scores = corpus.relevant_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "print(len(relevant_set))\n",
    "print(len(relevant_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lockheed Martin Corporation\n"
     ]
    }
   ],
   "source": [
    "print(tick.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3159287780355804, 0.3093508127026598, 0.3530262720185346, 0.30575258988418713, 0.30795690343681004, 0.39810053349194713]\n",
      "\n",
      "Lockheed Martin Corporation (NYSE: LMT) Trading Performance Indicators\n",
      "Let’s observe the current performance indicators for Lockheed Martin Corporation (LMT). It’s Quick Ratio in the last reported quarter now stands at 1.20. The Stock has managed to achieve an average true range (ATR) of 5.98. Alongside those numbers, its PE Ratio stands at $15.78, and its Beta score is 0.92. Another valuable indicator worth pondering is a publicly-traded company’s price to sales ratio for trailing twelve months, which is currently 1.41. Similarly, its price to free cash flow for trailing twelve months is now 42.16. \n",
      "In the same vein, LMT’s Diluted EPS (Earnings per Share) trailing twelve months is recorded 21.67, a figure that is expected to reach 7.17 in the next quarter, and analysts are predicting that it will be 26.45 at the market close of one year from today. \n",
      "Technical Analysis of Lockheed Martin Corporation (LMT)\n",
      "Going through the that latest performance of [Lockheed Martin Corporation, LMT]. Its last 5-days volume of 1.33 million was inferior to the volume of 1.5 million it revealed a year ago. During the previous 9 days, stock’s Stochastic %D was recorded 80.82% While, its Average True Range was 5.23.\n",
      " >> 7 Top Picks for the Post-Pandemic Economy << \n",
      "Raw Stochastic average of Lockheed Martin Corporation (LMT) in the period of the previous 100 days is set at 29.52%, which indicates a major fall in contrast to 91.96% during the last 2-weeks. If we go through the volatility metrics of the stock, In the past 14-days, Company’s historic volatility was 13.56% that was lower than 25.32% volatility it exhibited in the past 100-days period.\n"
     ]
    }
   ],
   "source": [
    "print(relevant_scores[5])\n",
    "print(relevant_set[5][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92653d83bd33458581cb5283a359b122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=143)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run the new relevant set through distilled-BERT and get sentiment classifications\n",
    "sentiments = get_sentiments(relevant_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.6789803504943848,\n",
       " -0.9776577949523926,\n",
       " -0.9890533089637756,\n",
       " 0.566616952419281,\n",
       " 0.6002805233001709,\n",
       " -0.9922448992729187]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens=[]\n",
    "for x in relevant_set.keys():\n",
    "    for y in range(0, len(relevant_set[x])):\n",
    "        lens.append(len(relevant_set[x][y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "avlen=np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_weight={}\n",
    "for x in relevant_set.keys():\n",
    "    l=[]\n",
    "    for y in range(0, len(relevant_set[x])):\n",
    "        l.append(len(relevant_set[x][y])/avlen)\n",
    "    len_weight[x]=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rel = {}\n",
    "for x in relevant_scores.keys():\n",
    "    l=[]\n",
    "    for y in range(0, len(relevant_scores[x])):\n",
    "        l.append(relevant_scores[x][y] * len_weight[x][y])\n",
    "    adjusted_rel[x]=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for x in relevant_set.keys():\n",
    "#     print(x, \"relevance:\", relevant_scores[x])\n",
    "#     print(\"  adjusted r:\", adjusted_rel[x])\n",
    "#     print(\"  sentiments:\", sentiments[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_scores=[]\n",
    "lrw_scores=[]\n",
    "for x in relevant_scores.keys():\n",
    "    for y in range(0, len(relevant_scores[x])):\n",
    "        rw_scores.append(relevant_scores[x][y] * sentiments[x][y])\n",
    "        lrw_scores.append(adjusted_rel[x][y] * sentiments[x][y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Relevance weighted Sentiment: -0.2161\n",
      "Average Length adjusted Relevance weighted sentiment: -0.2239\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Relevance weighted Sentiment:\", np.mean(rw_scores).round(4))\n",
    "print(\"Average Length adjusted Relevance weighted sentiment:\", np.mean(lrw_scores).round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
