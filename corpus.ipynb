{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from dateutil import parser as dparser\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy import sparse\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "import urllib.parse\n",
    "from util.config import config\n",
    "from util.pyBM25 import BM25\n",
    "from util.ticker import Ticker\n",
    "from util.web_query import web_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our classification model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "classifier = pipeline(task='sentiment-analysis',model=model,tokenizer=tokenizer)\n",
    "max_tokens = int(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running through sub_docs and outputting sentiment list\n",
    "\n",
    "def get_sentiments(docs):\n",
    "    c = 0\n",
    "    for x in docs.keys():\n",
    "        c+=len(docs[x])\n",
    "    \n",
    "    pgres = widgets.IntProgress(value=0,min=0,max=c, step=1)\n",
    "    display(pgres)\n",
    "    \n",
    "    sentiments = {}\n",
    "    \n",
    "    for x in docs.keys():\n",
    "        scrs=[]\n",
    "        for y in range(0, len(docs[x])):\n",
    "            s = classifier(docs[x][y])\n",
    "            scr = s[0]['score']\n",
    "            if s[0]['label']==\"NEGATIVE\":\n",
    "                scr=scr*-1\n",
    "            scrs.append(scr)\n",
    "            pgres.value+=1\n",
    "            pgres.description=str(pgres.value)+\":\"+str(c)\n",
    "        \n",
    "        sentiments[x]=scrs\n",
    "                \n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the rows of a 2d input_matrix so they sum to 1\n",
    "    \"\"\"\n",
    "\n",
    "    row_sums = input_matrix.sum(axis=1)\n",
    "    try:\n",
    "        assert (np.count_nonzero(row_sums)==np.shape(row_sums)[0]) # no row should sum to zero\n",
    "    except Exception:\n",
    "        raise Exception(\"Error while normalizing. Row(s) sum to zero\")\n",
    "    new_matrix = input_matrix / row_sums[:, np.newaxis]\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    #******************************************************************************\n",
    "    #----------------------------------Method---------------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    #Run the web_query to produce a collection of text documents scraped from the web\n",
    "    \n",
    "    #Use the set_results() function to store the full results in the corpus for processing\n",
    "    \n",
    "    #Use the set_corpus() function to assign the documents scraped from the web to the corpus\n",
    "    \n",
    "    #Sub divide the documents into smaller sub_docs\n",
    "    \n",
    "    #Rank the documents based on relevance to the original query as well as any tags\n",
    "    \n",
    "    #Prune the sub_docs to produce a relevant set\n",
    "    \n",
    "    \n",
    "    #******************************************************************************\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def __init__(self):\n",
    "        #typical corpus data\n",
    "        self.documents = []\n",
    "        self.vocabulary = []\n",
    "        self.number_of_documents = 0\n",
    "        self.vocabulary_size = 0\n",
    "        \n",
    "        #plsa and liklihoods\n",
    "        self.likelihoods = []\n",
    "        self.term_doc_matrix = None \n",
    "        self.document_topic_prob = None  # P(z | d)\n",
    "        self.topic_word_prob = None  # P(w | z)\n",
    "        self.topic_prob = None  # P(z | d, w)\n",
    "                \n",
    "        #instance fields for web results\n",
    "        self.query_results=None\n",
    "        self.max_tokens=512\n",
    "        self.failed = []\n",
    "        \n",
    "        #sub dividing documents\n",
    "        self.tokenizer=None\n",
    "        self.sub_docs=None\n",
    "        \n",
    "        #relevance scores\n",
    "        self.document_scores=None\n",
    "        self.document_tag_scores=None\n",
    "        self.subdoc_scores=None\n",
    "        self.subdoc_tag_scores=None\n",
    "        self.title_scores=None\n",
    "                \n",
    "        #pruned data\n",
    "        self.relevant_set=None\n",
    "        self.relevant_scores=None\n",
    "    \n",
    "    \n",
    "    #******************************************************************************\n",
    "    #------------------------------Setting Corpus----------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def set_results(self, df):\n",
    "        #dataframe returned from webquery\n",
    "        self.query_results=df\n",
    "    \n",
    "    def set_corpus(self, documents):\n",
    "        self.documents = documents\n",
    "        \n",
    "    def build_corpus_from_url(self, max_docs=50):\n",
    "        #scrape text from url-list to build corpus\n",
    "        #(not recommended, use the same method from the web_query object and the set_corpus() method)\n",
    "        \n",
    "        url_list = self.query_results['url'].tolist()\n",
    "        url_list = url_list[0:max_docs]\n",
    "        \n",
    "        pgres = widgets.IntProgress(value=0,min=0,max=len(url_list), step=1)\n",
    "        display(pgres)\n",
    "        \n",
    "        failed=[]\n",
    "        headers = {\"User-Agent\":\"Mozilla/5.0\"}\n",
    "        for i in range(0,len(url_list)):\n",
    "            try:\n",
    "                response = requests.get(url=url_list[i],headers=headers)\n",
    "                if response.status_code==200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    d = soup.get_text()\n",
    "                    if len(d)>200:\n",
    "                        self.documents.append(d)\n",
    "                else:\n",
    "                    self.failed.append(i)\n",
    "            except:\n",
    "                self.failed.append(i)\n",
    "\n",
    "            finally:\n",
    "                pgres.value+=1\n",
    "                pgres.description=str(i+1)+\":\"+str(len(url_list))\n",
    "                \n",
    "        self.number_of_documents=len(self.documents)\n",
    "        #remove failed url responses from dataset\n",
    "        self.query_results = self.query_results.take(list(set(range(self.query_results.shape[0]))-set(self.failed)))\n",
    "        \n",
    "    # Creates corpus & corresponding docs from inputted file\n",
    "    def build_corpus_from_file(self, file_path):\n",
    "        f = open(file_path, 'r')\n",
    "        docs = f.readlines()\n",
    "        for d in docs:\n",
    "            self.documents.append(d)\n",
    "        self.number_of_documents = len(docs)\n",
    "\n",
    "     # Augments the classifiers vocabulary   \n",
    "    def build_vocabulary(self, stopwords):\n",
    "        v = set([])\n",
    "        for x in self.documents:\n",
    "            tmp = set(x.split())\n",
    "            tmp = {x for x in tmp if x.lower() not in stopwords}\n",
    "                        \n",
    "            v.update(tmp)\n",
    "        \n",
    "        v = list(v)\n",
    "        self.vocabulary = v\n",
    "        self.vocabulary_size = len(v)\n",
    "        \n",
    "             \n",
    "    \n",
    "    #******************************************************************************\n",
    "    #------------------------------Sub Dividing-------------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def get_pgraphs(self, doc, cutoff, method):\n",
    "        #updated get_pgraphs() with method for cutoff\n",
    "        #cut off method:\n",
    "        #sen: number of sentences\n",
    "        #word: number of words  \n",
    "\n",
    "        pgraphs=[]\n",
    "        freshsoup = re.split('\\n\\n',doc)\n",
    "        for x in range(0,len(freshsoup)):\n",
    "            if method=='word':\n",
    "                words = len(str(freshsoup[x]).strip().split(' ',maxsplit=cutoff))\n",
    "                if words>cutoff:\n",
    "                    pgraphs.append(freshsoup[x])\n",
    "                    \n",
    "            elif method=='sen':\n",
    "                sens = len(re.findall(\"\\.\",str(freshsoup[x]).strip()))\n",
    "                if sens>cutoff:\n",
    "                    pgraphs.append(freshsoup[x])\n",
    "                    \n",
    "        return pgraphs\n",
    "    \n",
    "    def split_doc(self, doc, subs):         \n",
    "        if len(re.findall(r'\\.', doc))>1:\n",
    "            cut_point = doc.rfind('.', 0, int(len(doc)/2))+1\n",
    "        else:\n",
    "            cut_point = int(len(doc)/2)\n",
    "\n",
    "        d1 = doc[0:cut_point]\n",
    "        d2 = doc[cut_point+1:]\n",
    "\n",
    "        tkns1 = int(len(self.tokenizer(d1)['input_ids']))\n",
    "\n",
    "        if tkns1>self.max_tokens:\n",
    "            self.split_doc(d1,subs)\n",
    "        else:\n",
    "            if len(d1)>0:\n",
    "                subs.append(d1)\n",
    "\n",
    "        tkns2 = int(len(self.tokenizer(d2)['input_ids']))\n",
    "\n",
    "        if tkns2>self.max_tokens:\n",
    "            self.split_doc(d2, subs)\n",
    "        else:\n",
    "            if len(d2)>0:\n",
    "                subs.append(d2)\n",
    "            \n",
    "    \n",
    "    def get_subdocs(self, pgraphs):\n",
    "        #Updated get_subdocs with iterative slicing \n",
    "        #ensure sub_docs tokens will not exceed max_tokens for sentiment model\n",
    "        sub_docs=[]\n",
    "\n",
    "        for x in range(0, len(pgraphs)):\n",
    "            sen_cnt = len(re.split('\\n|\\. ',pgraphs[x]))\n",
    "            tkns = int(len(tokenizer(pgraphs[x])['input_ids']))\n",
    "\n",
    "            if tkns<self.max_tokens:\n",
    "                sub_docs.append(pgraphs[x])\n",
    "            else:\n",
    "                self.split_doc(pgraphs[x],sub_docs)\n",
    "        \n",
    "        return sub_docs\n",
    "        \n",
    "    def sub_divide(self, tokenizer, cutoff=1, method='sen'):\n",
    "        #creates a dictionary of sub_docs divided from each document in the corpus\n",
    "        #method: using get_pgraphs() followed by get_subdocs()\n",
    "        #output form: dict{ document_id : [subdoc_1, subdoc_2 ... subdoc_n] }\n",
    "\n",
    "        subbed_data = {}\n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "        for x in range(0, len(self.documents)):\n",
    "\n",
    "            pg = self.get_pgraphs(self.documents[x], cutoff, method)\n",
    "            subs = self.get_subdocs(pg)\n",
    "            subbed_data[x]=subs\n",
    "\n",
    "        self.sub_docs = subbed_data\n",
    "\n",
    "        \n",
    "    #******************************************************************************\n",
    "    #----------------------------------Relevance Scoring---------------------------\n",
    "    #******************************************************************************  \n",
    "    \n",
    "    def rank_docs(self, query, ranker):\n",
    "        self.document_scores = ranker.score(query, self.documents)\n",
    "        \n",
    "    def rank_doc_tags(self, tags, ranker):\n",
    "        tag_scores=[]\n",
    "        for t in tags:\n",
    "            scores = ranker.score(t, self.documents)\n",
    "            tag_scores.append(scores)\n",
    "            \n",
    "        self.document_tag_scores = tag_scores\n",
    "        \n",
    "    def rank_subdocs(self, query, ranker):\n",
    "        sub_vecs={}\n",
    "        for x in self.sub_docs.keys():\n",
    "            sub_vec = ranker.score(query, self.sub_docs[x])\n",
    "            sub_vecs[x]=sub_vec\n",
    "            \n",
    "        self.subdoc_scores = sub_vecs\n",
    "    \n",
    "    def rank_subdocs_tags(self, tags, ranker):\n",
    "        tag_scores=[]\n",
    "        for t in tags:\n",
    "            sub_vecs={}\n",
    "            for x in self.sub_docs.keys():\n",
    "                sub_vec = ranker.score(t, self.sub_docs[x])\n",
    "                sub_vecs[x]=sub_vec\n",
    "            tag_scores.append(sub_vecs)\n",
    "        \n",
    "        self.subdoc_tag_scores = tag_scores\n",
    "    \n",
    "    def rank_titles(self, name, ranker):\n",
    "        name = re.sub('(,|\\.|Inc| )',\"\",str(name))\n",
    "        titles = self.query_results['title'].tolist()\n",
    "        self.title_scores = ranker.score(name, titles)\n",
    "        \n",
    "    def rank_ticker(self, ticker, ranker):\n",
    "        #Takes a stock ticker object and runs all of the rankers above\n",
    "        \n",
    "        name = ticker.name\n",
    "        sym = ticker.ticker\n",
    "        tags = ticker.tags\n",
    "        \n",
    "        self.rank_docs(name,ranker)\n",
    "        self.rank_doc_tags(tags, ranker)\n",
    "        self.rank_subdocs(name,ranker)\n",
    "        self.rank_subdocs_tags(tags,ranker)\n",
    "        self.rank_titles(name,ranker)\n",
    "        \n",
    "    #******************************************************************************\n",
    "    #----------------------------Pruning Relevant Set------------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    def prune_subdocs(self, cutoff=0.4):\n",
    "        subbed_data = self.sub_docs\n",
    "        sub_scores = self.subdoc_scores\n",
    "        for x in self.sub_docs.keys():\n",
    "\n",
    "            subbed_data[x] = [xv if c else None for c, xv in zip(sub_scores[x]>cutoff, subbed_data[x])]\n",
    "            subbed_data[x] = [y for y in subbed_data[x] if y!=None]\n",
    "            sub_scores[x] = [y for y in sub_scores[x] if y>cutoff]\n",
    "        \n",
    "        self.relevant_set = {k: v for k, v in subbed_data.items() if len(v) > 0}\n",
    "        self.relevant_scores={k: v for k, v in sub_scores.items() if len(v) > 0}\n",
    "    \n",
    "    #******************************************************************************\n",
    "    #-------------------------------------PLSA (from MP3)--------------------------\n",
    "    #******************************************************************************\n",
    "    \n",
    "    # Use MP3 as a reference\n",
    "    def build_term_doc_matrix(self):\n",
    "        m = []\n",
    "        line = []\n",
    "        for x in self.documents:\n",
    "            doc = list(x.split())\n",
    "            for itm in self.vocabulary:\n",
    "                line.append(x.count(itm))\n",
    "            m.append(line)\n",
    "            line = []\n",
    "        self.term_doc_matrix = np.array(m)\n",
    "        \n",
    "    def initialize_prob(self, number_of_topics):\n",
    "        self.document_topic_prob = np.random.random_sample((self.number_of_documents, number_of_topics))\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "\n",
    "        self.topic_word_prob = np.random.random_sample((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "\n",
    "            \n",
    "    def E_step(self):\n",
    "        for x in range(0,self.term_doc_matrix.shape[0]):  #loop through documents\n",
    "            e = self.document_topic_prob[x].reshape(-1,1)*self.topic_word_prob\n",
    "            self.topic_prob[x] = normalize(e)\n",
    "           \n",
    "\n",
    "    def M_step(self, number_of_topics):\n",
    "        pz = []\n",
    "        for x in range(0, self.term_doc_matrix.shape[0]):         \n",
    "            m = self.topic_prob[x]*self.term_doc_matrix[x].reshape(1,-1)\n",
    "            self.document_topic_prob[x] = np.sum(m,axis=1)\n",
    "            pz.append(m)\n",
    "        \n",
    "        pz = np.array(pz)\n",
    "        self.topic_word_prob = np.sum(pz,axis=0)\n",
    "        \n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    " \n",
    "\n",
    "    def calculate_likelihood(self, number_of_topics):\n",
    "        l = np.log(np.prod(np.power(np.dot(self.document_topic_prob,self.topic_word_prob),self.term_doc_matrix),axis=1))\n",
    "        l = l[np.argmax(l)]\n",
    "        self.likelihoods.append(l)\n",
    "        \n",
    "\n",
    "    def plsa(self, number_of_topics, max_iter, epsilon):\n",
    "        self.build_term_doc_matrix()\n",
    "        self.topic_prob = np.zeros([self.number_of_documents, number_of_topics, self.vocabulary_size], dtype=np.float)\n",
    "        self.initialize_prob(number_of_topics)\n",
    "        current_likelihood = 0.0\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "            self.E_step()\n",
    "            self.M_step(number_of_topics)\n",
    "            \n",
    "            l = self.calculate_likelihood(number_of_topics)\n",
    "            \n",
    "            if current_likelihood==0 or current_likelihood==None or l>current_likelihood:\n",
    "                current_likelihood = l\n",
    "            else:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull api keys from the config file\n",
    "cfig=config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a ticker object \n",
    "tick = Ticker(cfig, \"LMT\",source='yahoo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq=web_query(cfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87df1cee21914ec5b2c83508afc7ecf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=200)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testq=tick.name\n",
    "d_start=\"11/1/2021\"\n",
    "#query all of the news apis in web_query object\n",
    "wq.query_all(query=tick.name, ticker=tick.ticker, d_start=d_start)\n",
    "#compile results into a singular dataframe\n",
    "wq.compile_results()\n",
    "#scrap text from the results urls to form documents\n",
    "wq.scrape_results(threaded=True, max_docs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wq.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build corpus from web query results\n",
    "corpus=Corpus()\n",
    "#store the web query data frame in the corpus for referencing urls and titles\n",
    "corpus.set_results(df)\n",
    "#assign corpus documents as the web query documents\n",
    "corpus.set_corpus(wq.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:22570\n",
      "Number of documents:162\n"
     ]
    }
   ],
   "source": [
    "#pull in stop words and build corpus vocabulary \n",
    "stopwords=[]\n",
    "with open('util/stopwords.txt') as f:\n",
    "    stopwords.append(f.read().splitlines())\n",
    "stopwords=stopwords[0]\n",
    "\n",
    "corpus.build_vocabulary(stopwords)\n",
    "\n",
    "print(\"Vocabulary size:\" + str(len(corpus.vocabulary)))\n",
    "print(\"Number of documents:\" + str(len(corpus.documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build BM25 ranker fit to the corpus vocabulary\n",
    "bm25 = BM25(norm='l2', smooth_idf=True, stopwords=stopwords, sublinear_tf=True, vocabulary=corpus.vocabulary)\n",
    "bm25.fit(corpus.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-docs: 162\n"
     ]
    }
   ],
   "source": [
    "#create the sub_documents, wrapper to run multiple functions\n",
    "#passing in the tokenizer to save a little on class dependencies\n",
    "corpus.sub_divide(tokenizer=tokenizer, cutoff=2, method='sen')\n",
    "print('Sub-docs:',len(corpus.sub_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass in ticker object and use the BM25 ranker to do a collection of ranking\n",
    "#this is the same as running each commented function below one by one\n",
    "\n",
    "corpus.rank_ticker(tick,bm25)\n",
    "\n",
    "# corpus.rank_docs(tick.name, bm25)\n",
    "# corpus.rank_doc_tags(tick.tags, bm25)\n",
    "# corpus.rank_subdocs(tick.name, bm25)\n",
    "# corpus.rank_subdocs_tags(tick.tags, bm25)\n",
    "# corpus.rank_titles(tick.name,bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 'relevant set' by pruning the sub_doc based on a cutoff value for the ranker score\n",
    "corpus.prune_subdocs(cutoff=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_set = corpus.relevant_set\n",
    "relevant_scores = corpus.relevant_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "print(len(relevant_set))\n",
    "print(len(relevant_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lockheed Martin Corporation\n"
     ]
    }
   ],
   "source": [
    "print(tick.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28445012768236494]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9297ffa5e9dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelevant_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelevant_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(relevant_scores[5])\n",
    "print(relevant_set[5][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92653d83bd33458581cb5283a359b122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=143)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run the new relevant set through distilled-BERT and get sentiment classifications\n",
    "sentiments = get_sentiments(relevant_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.6789803504943848,\n",
       " -0.9776577949523926,\n",
       " -0.9890533089637756,\n",
       " 0.566616952419281,\n",
       " 0.6002805233001709,\n",
       " -0.9922448992729187]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens=[]\n",
    "for x in relevant_set.keys():\n",
    "    for y in range(0, len(relevant_set[x])):\n",
    "        lens.append(len(relevant_set[x][y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avlen=np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate all relevance weights\n",
    "len_weight={}\n",
    "for x in relevant_set.keys():\n",
    "    l=[]\n",
    "    for y in range(0, len(relevant_set[x])):\n",
    "        l.append(len(relevant_set[x][y])/avlen)\n",
    "    len_weight[x]=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust relevance weight based on length\n",
    "adjusted_rel = {}\n",
    "for x in relevant_scores.keys():\n",
    "    l=[]\n",
    "    for y in range(0, len(relevant_scores[x])):\n",
    "        l.append(relevant_scores[x][y] * len_weight[x][y])\n",
    "    adjusted_rel[x]=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for x in relevant_set.keys():\n",
    "#     print(x, \"relevance:\", relevant_scores[x])\n",
    "#     print(\"  adjusted r:\", adjusted_rel[x])\n",
    "#     print(\"  sentiments:\", sentiments[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_scores=[]\n",
    "lrw_scores=[]\n",
    "for x in relevant_scores.keys():\n",
    "    for y in range(0, len(relevant_scores[x])):\n",
    "        rw_scores.append(relevant_scores[x][y] * sentiments[x][y])\n",
    "        lrw_scores.append(adjusted_rel[x][y] * sentiments[x][y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Relevance weighted Sentiment: -0.2161\n",
      "Average Length adjusted Relevance weighted sentiment: -0.2239\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Relevance weighted Sentiment:\", np.mean(rw_scores).round(4))\n",
    "print(\"Average Length adjusted Relevance weighted sentiment:\", np.mean(lrw_scores).round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
