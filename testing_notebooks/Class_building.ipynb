{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dateutil import parser as dparser\n",
    "import urllib.parse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy import sparse\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from util.config import config\n",
    "from util.pyBM25 import BM25\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "classifier = pipeline(task='sentiment-analysis',model=model,tokenizer=tokenizer)\n",
    "max_tokens = int(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ticker(object):\n",
    "    \n",
    "    def __init__(self,config, t):\n",
    "        \n",
    "        #Get general information about company from ticker symbol via api\n",
    "        #name, ticker, industry, sector, tags\n",
    "        \n",
    "        t = t.upper()\n",
    "        \n",
    "        #Try to get ticker information from Polygon.io\n",
    "        url = ('https://api.polygon.io/v1/meta/symbols/'+t+'/company?apikey='+config.polygon)\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code==200:\n",
    "            response = response.json()\n",
    "            self.name = response['name']\n",
    "            self.tags = response['tags']\n",
    "            self.industry = response['industry']\n",
    "            self.sector = response['sector']\n",
    "            self.ticker = response['symbol']\n",
    "        else:\n",
    "            #Try to get ticker information from yahoofinance\n",
    "            print(\"Bad reponse from Polygon.io: Status_code:\"+str(response.status_code))\n",
    "            print(\"Trying Yahoo Api\")\n",
    "\n",
    "            yheaders = {'x-api-key':config.yahoo}\n",
    "            url = ('https://yfapi.net/v11/finance/quoteSummary/'+t+'?lang=en&region=US&modules=assetProfile')\n",
    "            url2 = ('https://yfapi.net/v6/finance/quote?symbols='+t)\n",
    "            response1 = requests.request(\"GET\",url,headers=yheaders).json()\n",
    "            response2 = requests.request(\"GET\",url2,headers=yheaders).json()\n",
    "            \n",
    "            self.sector = response1['quoteSummary'].get('result')[0].get('assetProfile').get('sector')\n",
    "            self.industry = response1['quoteSummary'].get('result')[0].get('assetProfile').get('industry')\n",
    "            self.name = response2['quoteResponse'].get('result')[0].get('longName')\n",
    "            self.ticker = response2['quoteResponse'].get('result')[0].get('symbol')\n",
    "            t1 = re.split('\\n|\\.|&|,|and',self.sector)\n",
    "            t2 = re.split('\\n|\\.|&|,|and',self.industry)\n",
    "            tags=[]\n",
    "            for i in t1:\n",
    "                tags.append(i.strip())\n",
    "            for i in t2:\n",
    "                tags.append(i.strip())\n",
    "            self.tags=tags\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_Usearch(config, query, d_start='Now', d_end='Now', page=1, pageSize=50):\n",
    "    \n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    #Valid format : Date format should be YYYY-MM-ddTHH:mm:ss.ss±hh:mm\n",
    "    if d_end=='Now':\n",
    "        d_end=datetime.now()\n",
    "    else:\n",
    "        d_end = dparser.parse(d_end)\n",
    "    d_end_str = d_end.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "    if d_start=='Now':\n",
    "        d_start=datetime.now()\n",
    "    else:\n",
    "        d_start = dparser.parse(d_start)\n",
    "    d_start_str = d_start.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    url = \"https://contextualwebsearch-websearch-v1.p.rapidapi.com/api/search/NewsSearchAPI\"\n",
    "    querystring = {\"q\":query,\n",
    "                   \"pageNumber\":\"1\",\n",
    "                   \"pageSize\":\"150\",\n",
    "                   \"autoCorrect\":\"false\",\n",
    "                   \"fromPublishedDate\":d_start,\n",
    "                   \"toPublishedDate\":d_end}\n",
    "\n",
    "    headers = {\n",
    "        'x-rapidapi-host': str(config.usearch_host),\n",
    "        'x-rapidapi-key': str(config.usearch_key)\n",
    "        }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    \n",
    "    #Check that reponse is valid\n",
    "    if response.status_code==200:\n",
    "        response = response.json()\n",
    "        \n",
    "        #Loop through response to create return Dataframe: columns: Titles, Urls, Publication Dates\n",
    "        datalist=[]\n",
    "        for x in response['value']:\n",
    "            row = {'title':str(x['title']), 'url':x['url'], 'pub_date':x['datePublished']}\n",
    "            datalist.append(row)\n",
    "        df = pd.DataFrame.from_dict(datalist)\n",
    "        return df\n",
    "    else:\n",
    "        return 0      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_currents(config, query, d_start='Now', d_end='Now', page=1, pageSize=200):\n",
    "    \n",
    "    #6-Month archive\n",
    "    \n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    \n",
    "    #Valid format : Date format should be YYYY-MM-ddTHH:mm:ss.ss±hh:mm\n",
    "    if d_end=='Now':\n",
    "        d_end=datetime.now()\n",
    "    else:\n",
    "        d_end = dparser.parse(d_end)\n",
    "    d_end_str = d_end.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "    if d_start=='Now':\n",
    "        d_start=datetime.now()\n",
    "    else:\n",
    "        d_start = dparser.parse(d_start)\n",
    "    d_start_str = d_start.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    \n",
    "    url = ('https://api.currentsapi.services/v1/search?'\n",
    "           '&start_date='+d_start_str+\n",
    "           '&end_date='+d_end_str+\n",
    "           '&keywords='+query+\n",
    "           '&language=en'\n",
    "           '&country=us'\n",
    "           '&page_number='+str(page)+\n",
    "           '&page_size='+str(pageSize)+\n",
    "           '&apiKey='+str(config.currents))\n",
    "    \n",
    "    #Check that reponse is valid\n",
    "    response = requests.get(url)\n",
    "    if response.status_code==200:\n",
    "        response = response.json()\n",
    "        \n",
    "        #Loop through response to create return Dataframe: columns: Titles, Urls, Publication Dates\n",
    "        datalist=[]\n",
    "        for x in response['news']:\n",
    "            row = {'title':str(x['title']), 'url':x['url'], 'pub_date':x['published']}\n",
    "            datalist.append(row)\n",
    "        df = pd.DataFrame.from_dict(datalist)\n",
    "        return df\n",
    "    else:\n",
    "        return 0      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_polygon(config, ticker, d_start='Now', d_end='Now', pageSize=200):\n",
    "    \n",
    "    ticker = ticker.upper()\n",
    "    \n",
    "    #Valid format : Date format should be YYYY-MM-ddTHH:mm:ss.ss±hh:mm\n",
    "    if d_end=='Now':\n",
    "        d_end=datetime.now()\n",
    "    else:\n",
    "        d_end = dparser.parse(d_end)\n",
    "    d_end_str = d_end.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "    if d_start=='Now':\n",
    "        d_start=datetime.now()\n",
    "    else:\n",
    "        d_start = dparser.parse(d_start)\n",
    "    d_start_str = d_start.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    \n",
    "    url = ('https://api.polygon.io/v2/reference/news?'\n",
    "      'ticker='+ticker+\n",
    "      '&published_utc/gte='+d_start_str+\n",
    "      '&published_utc/lte='+d_end_str+\n",
    "      '&limit='+str(pageSize)+\n",
    "      '&sort=published_utc'\n",
    "      '&apikey='+str(config.polygon))\n",
    "    \n",
    "    #Check that reponse is valid\n",
    "    response = requests.get(url)\n",
    "    if response.status_code==200:\n",
    "        response = response.json()\n",
    "        \n",
    "        #Loop through response to create return Dataframe: columns: Titles, Urls, Publication Dates\n",
    "        datalist=[]\n",
    "        for x in response['results']:\n",
    "            row = {'title':str(x['title']), 'url':x['article_url'], 'pub_date':x['published_utc']}\n",
    "            datalist.append(row)\n",
    "        df = pd.DataFrame.from_dict(datalist)\n",
    "        return df\n",
    "    else:\n",
    "        return 0     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_newsapi(config, query, d_start='Now', d_end='Now', domains=\"\", exclude=\"\", page=1, pageSize=100):\n",
    "    \n",
    "    #1 Month archive\n",
    "    \n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    \n",
    "    #Valid format : Date format should be YYYY-MM-ddTHH:mm:ss.ss±hh:mm\n",
    "    if d_end=='Now':\n",
    "        d_end=datetime.now()\n",
    "    else:\n",
    "        d_end = dparser.parse(d_end)\n",
    "    d_end_str = d_end.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "    if d_start=='Now':\n",
    "        d_start=datetime.now()\n",
    "    else:\n",
    "        d_start = dparser.parse(d_start)\n",
    "    d_start_str = d_start.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    url = ('https://newsapi.org/v2/everything?'\n",
    "      'q='+query+\n",
    "      '&domains='+domains+\n",
    "      '&excludeDomains='+exclude+\n",
    "      '&from='+d_start_str+\n",
    "      '&to='+d_end_str+\n",
    "      '&language=en'\n",
    "      '&sortBy=publishedAt'\n",
    "      '&pageSize='+str(pageSize)+\n",
    "      '&page='+str(page)+  \n",
    "      '&apikey='+str(config.newsapi))\n",
    "    \n",
    "    #Check that reponse is valid\n",
    "    response = requests.get(url)\n",
    "    if response.status_code==200:\n",
    "        response = response.json()\n",
    "        \n",
    "        #Loop through response to create return Dataframe: columns: Titles, Urls, Publication Dates    \n",
    "        datalist=[]\n",
    "        for x in response['articles']:\n",
    "            row = {'title':str(x['title']), 'url':x['url'], 'pub_date':x['publishedAt']}\n",
    "            datalist.append(row)\n",
    "        df = pd.DataFrame.from_dict(datalist)\n",
    "        return df\n",
    "    else:\n",
    "        return 0    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plug Power Inc.\n"
     ]
    }
   ],
   "source": [
    "tick = Ticker(config,'PLUG')\n",
    "print(tick.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Query\n",
    "testq=\"PLUG\"\n",
    "d_start=\"11/1/2021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running query through Usearch API\n",
    "usearch_df = query_Usearch(config=config, query=testq, d_start=d_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running query through Currents API\n",
    "currents_df = query_currents(config=config, query=testq, d_start=d_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running query through Polygon.io\n",
    "polygon_df = query_polygon(config=config, ticker=testq, d_start=d_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running query through (google)NewsAPI\n",
    "newsapi_df = query_newsapi(config=config, query=testq, d_start=d_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine results into singular dataframe\n",
    "#Remove duplicate urls and article titles\n",
    "frames = [usearch_df, currents_df, polygon_df, newsapi_df]\n",
    "total_df = pd.concat(frames)\n",
    "total_df = total_df.drop_duplicates(subset=['url'])\n",
    "total_df = total_df.drop_duplicates(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>pub_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Floundering smart meter rollout bids to plug i...</td>\n",
       "      <td>https://www.telegraph.co.uk/business/2021/11/1...</td>\n",
       "      <td>2021-11-13T12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rover looks at EV rates for charging, gas tax ...</td>\n",
       "      <td>https://www.cadillacnews.com/news/rover-looks-...</td>\n",
       "      <td>2021-11-13T07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Plug in</td>\n",
       "      <td>https://www.wenatcheeworld.com/news/plug-in/ar...</td>\n",
       "      <td>2021-11-12T18:11:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon Smart Plug review: Everything should be...</td>\n",
       "      <td>https://www.androidcentral.com/amazon-smart-pl...</td>\n",
       "      <td>2021-11-12T16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France: Plug-In Market Share Reaches 22.9% In ...</td>\n",
       "      <td>https://insideevs.com/news/547155/france-plugi...</td>\n",
       "      <td>2021-11-12T13:30:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Officials worry China’s men’s hockey team not ...</td>\n",
       "      <td>https://www.denverpost.com/2021/11/12/beijing-...</td>\n",
       "      <td>2021-11-13T00:43:43Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Magazine Luiza SA (MGLUY) CEO Frederico Trajan...</td>\n",
       "      <td>https://seekingalpha.com/article/4469095-magaz...</td>\n",
       "      <td>2021-11-13T00:41:05Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>PowerSchool Holdings Inc. (PWSC) CEO Hardeep G...</td>\n",
       "      <td>https://seekingalpha.com/article/4469092-power...</td>\n",
       "      <td>2021-11-13T00:34:01Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>CLEC Fashion Festival Reimagines Fashion As An...</td>\n",
       "      <td>https://www.forbes.com/sites/stephanrabimov/20...</td>\n",
       "      <td>2021-11-13T00:18:58Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Greenbrook TMS, Inc. (GBNH) CEO William Leonar...</td>\n",
       "      <td>https://seekingalpha.com/article/4469090-green...</td>\n",
       "      <td>2021-11-13T00:13:06Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>489 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Floundering smart meter rollout bids to plug i...   \n",
       "1   Rover looks at EV rates for charging, gas tax ...   \n",
       "2                                             Plug in   \n",
       "3   Amazon Smart Plug review: Everything should be...   \n",
       "4   France: Plug-In Market Share Reaches 22.9% In ...   \n",
       "..                                                ...   \n",
       "94  Officials worry China’s men’s hockey team not ...   \n",
       "95  Magazine Luiza SA (MGLUY) CEO Frederico Trajan...   \n",
       "96  PowerSchool Holdings Inc. (PWSC) CEO Hardeep G...   \n",
       "97  CLEC Fashion Festival Reimagines Fashion As An...   \n",
       "98  Greenbrook TMS, Inc. (GBNH) CEO William Leonar...   \n",
       "\n",
       "                                                  url              pub_date  \n",
       "0   https://www.telegraph.co.uk/business/2021/11/1...   2021-11-13T12:00:00  \n",
       "1   https://www.cadillacnews.com/news/rover-looks-...   2021-11-13T07:00:00  \n",
       "2   https://www.wenatcheeworld.com/news/plug-in/ar...   2021-11-12T18:11:00  \n",
       "3   https://www.androidcentral.com/amazon-smart-pl...   2021-11-12T16:00:00  \n",
       "4   https://insideevs.com/news/547155/france-plugi...   2021-11-12T13:30:16  \n",
       "..                                                ...                   ...  \n",
       "94  https://www.denverpost.com/2021/11/12/beijing-...  2021-11-13T00:43:43Z  \n",
       "95  https://seekingalpha.com/article/4469095-magaz...  2021-11-13T00:41:05Z  \n",
       "96  https://seekingalpha.com/article/4469092-power...  2021-11-13T00:34:01Z  \n",
       "97  https://www.forbes.com/sites/stephanrabimov/20...  2021-11-13T00:18:58Z  \n",
       "98  https://seekingalpha.com/article/4469090-green...  2021-11-13T00:13:06Z  \n",
       "\n",
       "[489 rows x 3 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can take a little while, added loading bar\n",
    "\n",
    "def build_corpus_from_url(url_list):\n",
    "    pgres = widgets.IntProgress(value=0,min=0,max=len(url_list), step=1)\n",
    "    display(pgres)\n",
    "    corpus = []\n",
    "    failed=[]\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0\"}\n",
    "    for i in range(0,len(url_list)):\n",
    "        try:\n",
    "            response = requests.get(url=url_list[i],headers=headers)\n",
    "            if response.status_code==200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                corpus.append(soup.get_text())\n",
    "            else:\n",
    "                #print(\"failed:\",i,url_list[i])\n",
    "                failed.append(i)\n",
    "        except:\n",
    "            #print(\"failed:\",i,url_list[i])\n",
    "            failed.append(i)\n",
    "            \n",
    "        finally:\n",
    "            pgres.value+=1\n",
    "            pgres.description=str(i+1)+\":\"+str(len(url_list))\n",
    "    return [corpus,failed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cutoff = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=total_df['url'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ad546635d04cf08f05829ae8c83afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = build_corpus_from_url(urls[0:test_cutoff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test dataframe of from full dataframe\n",
    "test_df=total_df[0:test_cutoff]\n",
    "\n",
    "#removing the failed url requests \n",
    "#->returned from build_corpus_from_url()\n",
    "test_df = test_df.take(list(set(range(test_df.shape[0]))-set(corpus[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus[0]))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=[]\n",
    "with open('util/stopwords.txt') as f:\n",
    "    stopwords.append(f.read().splitlines())\n",
    "stopwords=stopwords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str=\"This is a Test string for the purpose of removing stopwords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stopwords', 'for', 'is', 'purpose', 'This', 'removing', 'the', 'Test', 'string', 'of', 'a'}\n"
     ]
    }
   ],
   "source": [
    "tmp = set(test_str.split())\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stopwords', 'purpose', 'string', 'removing', 'Test'}\n"
     ]
    }
   ],
   "source": [
    "t = {x for x in tmp if x.lower() not in stopwords}\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tony\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doesn'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98547195 0.34162697 0.29910872 1.43847351 0.59897335 1.84991387\n",
      " 0.51436637 1.22435046 1.59984558 0.3533325  0.53195264 1.32095828\n",
      " 0.33871382 1.52009905 0.95021817 0.50973101 0.         0.28548806\n",
      " 0.         1.41492307 0.58077216 2.02190232 0.55133982 1.56727157\n",
      " 0.59561628 1.16509876 0.42234909 1.4997267  1.50403793 1.79830238\n",
      " 1.79917616 1.81698373 0.5134271  0.57604022 0.36593254 0.54200275\n",
      " 1.92171497 1.33591207 0.583196   1.65621447 2.0413675  1.08885524\n",
      " 2.003347   0.         0.29686605 0.3200048  0.48163607 0.37196921\n",
      " 1.67166616 0.20524894 0.9029974  1.12995467 0.52258901 0.\n",
      " 1.80096127 0.         1.92329135 1.14053577 0.40521273 0.\n",
      " 1.16515589 0.23793685 1.90204236 0.         0.         0.3985973\n",
      " 0.         0.         1.86684439 0.         2.04855942 0.75992987\n",
      " 0.         1.90942036 1.73114042 1.01668499 1.97837085 1.15816666\n",
      " 0.         0.         2.02616796 0.69472286 0.34276685 1.99608785\n",
      " 0.50059476 0.         1.21981665 1.00901508 1.29109599 1.49460143\n",
      " 1.18593543 1.6976778 ]\n"
     ]
    }
   ],
   "source": [
    "#Fitting BM25 ranker to full Corpus\n",
    "#Getting preliminary BM25 scores for full web scrape\n",
    "q = tick.name\n",
    "bm25 = BM25(stopwords=stopwords[0])\n",
    "bm25.fit(corpus[0])\n",
    "result_vec = bm25.transform(q, corpus[0])\n",
    "print(result_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['url_bm25']=result_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through tags (retrieved or created by Ticker class) associated with ticker and add tag-bm25 score to df\n",
    "#Add results into dataframe\n",
    "\n",
    "for x in range(0,len(tick.tags)):\n",
    "    q = tick.tags[x]\n",
    "    result_vec = bm25.transform(q, corpus[0])\n",
    "    col_str = \"tag\"+str(x)+\"_bm25\"\n",
    "    test_df[col_str]=result_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>url_bm25</th>\n",
       "      <th>tag0_bm25</th>\n",
       "      <th>tag1_bm25</th>\n",
       "      <th>tag2_bm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Floundering smart meter rollout bids to plug i...</td>\n",
       "      <td>https://www.telegraph.co.uk/business/2021/11/1...</td>\n",
       "      <td>2021-11-13T12:00:00</td>\n",
       "      <td>0.985472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rover looks at EV rates for charging, gas tax ...</td>\n",
       "      <td>https://www.cadillacnews.com/news/rover-looks-...</td>\n",
       "      <td>2021-11-13T07:00:00</td>\n",
       "      <td>0.341627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.212997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Plug in</td>\n",
       "      <td>https://www.wenatcheeworld.com/news/plug-in/ar...</td>\n",
       "      <td>2021-11-12T18:11:00</td>\n",
       "      <td>0.299109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon Smart Plug review: Everything should be...</td>\n",
       "      <td>https://www.androidcentral.com/amazon-smart-pl...</td>\n",
       "      <td>2021-11-12T16:00:00</td>\n",
       "      <td>1.438474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.220796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France: Plug-In Market Share Reaches 22.9% In ...</td>\n",
       "      <td>https://insideevs.com/news/547155/france-plugi...</td>\n",
       "      <td>2021-11-12T13:30:16</td>\n",
       "      <td>0.598973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Save Foods Inc. Announces Expansion of Commerc...</td>\n",
       "      <td>https://www.benzinga.com/news/21/11/23988067/s...</td>\n",
       "      <td>2021-11-09 16:43:49 +0000</td>\n",
       "      <td>1.009015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Karcher EWM 2</td>\n",
       "      <td>https://www.trustedreviews.com/reviews/karcher...</td>\n",
       "      <td>2021-11-09 13:25:52 +0000</td>\n",
       "      <td>1.291096</td>\n",
       "      <td>1.521352</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.150439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>SilverStone HELA 2050 Power Supply is Hella Po...</td>\n",
       "      <td>https://www.techpowerup.com/288795/silverstone...</td>\n",
       "      <td>2021-11-09 07:09:02 +0000</td>\n",
       "      <td>1.494601</td>\n",
       "      <td>0.761077</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.745432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>(PR) KIOXIA XD6 EDSFF E1.S Form-Factor Enterpr...</td>\n",
       "      <td>https://www.techpowerup.com/288789/kioxia-xd6-...</td>\n",
       "      <td>2021-11-09 04:09:21 +0000</td>\n",
       "      <td>1.185935</td>\n",
       "      <td>1.638326</td>\n",
       "      <td>2.71468</td>\n",
       "      <td>6.313200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Ballard Reports Q3 2021 Results</td>\n",
       "      <td>http://feeds.benzinga.com/~r/benzinga/news/ear...</td>\n",
       "      <td>2021-11-09 03:01:00 +0000</td>\n",
       "      <td>1.697678</td>\n",
       "      <td>1.843739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Floundering smart meter rollout bids to plug i...   \n",
       "1   Rover looks at EV rates for charging, gas tax ...   \n",
       "2                                             Plug in   \n",
       "3   Amazon Smart Plug review: Everything should be...   \n",
       "4   France: Plug-In Market Share Reaches 22.9% In ...   \n",
       "..                                                ...   \n",
       "57  Save Foods Inc. Announces Expansion of Commerc...   \n",
       "60                                      Karcher EWM 2   \n",
       "61  SilverStone HELA 2050 Power Supply is Hella Po...   \n",
       "62  (PR) KIOXIA XD6 EDSFF E1.S Form-Factor Enterpr...   \n",
       "63                    Ballard Reports Q3 2021 Results   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://www.telegraph.co.uk/business/2021/11/1...   \n",
       "1   https://www.cadillacnews.com/news/rover-looks-...   \n",
       "2   https://www.wenatcheeworld.com/news/plug-in/ar...   \n",
       "3   https://www.androidcentral.com/amazon-smart-pl...   \n",
       "4   https://insideevs.com/news/547155/france-plugi...   \n",
       "..                                                ...   \n",
       "57  https://www.benzinga.com/news/21/11/23988067/s...   \n",
       "60  https://www.trustedreviews.com/reviews/karcher...   \n",
       "61  https://www.techpowerup.com/288795/silverstone...   \n",
       "62  https://www.techpowerup.com/288789/kioxia-xd6-...   \n",
       "63  http://feeds.benzinga.com/~r/benzinga/news/ear...   \n",
       "\n",
       "                     pub_date  url_bm25  tag0_bm25  tag1_bm25  tag2_bm25  \n",
       "0         2021-11-13T12:00:00  0.985472   0.000000    0.00000   0.000000  \n",
       "1         2021-11-13T07:00:00  0.341627   0.000000    0.00000   2.212997  \n",
       "2         2021-11-12T18:11:00  0.299109   0.000000    0.00000   0.000000  \n",
       "3         2021-11-12T16:00:00  1.438474   0.000000    0.00000   5.220796  \n",
       "4         2021-11-12T13:30:16  0.598973   0.000000    0.00000   0.000000  \n",
       "..                        ...       ...        ...        ...        ...  \n",
       "57  2021-11-09 16:43:49 +0000  1.009015   0.000000    0.00000   0.000000  \n",
       "60  2021-11-09 13:25:52 +0000  1.291096   1.521352    0.00000   2.150439  \n",
       "61  2021-11-09 07:09:02 +0000  1.494601   0.761077    0.00000   5.745432  \n",
       "62  2021-11-09 04:09:21 +0000  1.185935   1.638326    2.71468   6.313200  \n",
       "63  2021-11-09 03:01:00 +0000  1.697678   1.843739    0.00000   0.000000  \n",
       "\n",
       "[77 rows x 7 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropping instances where the whole document's bm25 score was 0\n",
    "test_df = test_df[test_df['url_bm25']>0]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated get_pgraphs() with method for cutoff\n",
    "\n",
    "def get_pgraphs(doc, cutoff=7, method='word'):\n",
    "    \n",
    "    #cut off method:\n",
    "    #sen: number of sentences\n",
    "    #word: number of words  \n",
    "    \n",
    "    pgraphs=[]\n",
    "    freshsoup = re.split('\\n\\n',doc)\n",
    "    for x in range(0,len(freshsoup)):\n",
    "        if method=='word':\n",
    "            words = len(str(freshsoup[x]).strip().split(' ',maxsplit=cutoff))\n",
    "            if words>cutoff:\n",
    "                pgraphs.append(freshsoup[x])\n",
    "        elif method=='sen':\n",
    "            sens = len(re.findall(\"\\.\",str(freshsoup[x]).strip()))\n",
    "            if sens>cutoff:\n",
    "                pgraphs.append(freshsoup[x])\n",
    "    \n",
    "    return pgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing get_prgraphs\n",
    "doc = corpus[0][0]\n",
    "pgs = get_pgraphs(doc, cutoff=1, method='sen')\n",
    "print(\"Number of parsed sections:\",len(pgs))\n",
    "print(pgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated get_subdocs to ensure sub_docs tokens will not exceed max\n",
    "\n",
    "def get_subdocs(pgraphs, max_tokens):\n",
    "    sub_docs=[]\n",
    "\n",
    "    for x in range(0, len(pgraphs)):\n",
    "        sen_cnt = len(re.split('\\n|\\. ',pgraphs[x]))\n",
    "        tkns = int(len(tokenizer(pgraphs[x])['input_ids']))\n",
    "        \n",
    "        if tkns>=max_tokens:\n",
    "            \n",
    "            pg = pgraphs[x]\n",
    "            slices=0\n",
    "            \n",
    "            while True:\n",
    "                #cut in half, count tokens\n",
    "                slices+=1\n",
    "                cut_point = pg.rfind(\".\",0,int(len(pg)/2))+1\n",
    "                cut_tkns = int(len(tokenizer(pg[0:cut_point])['input_ids']))    \n",
    "            \n",
    "                if cut_tkns<max_tokens:\n",
    "                    break\n",
    "                else:\n",
    "                    #trim pg and recut, counting slices\n",
    "                    pg = pg[0:cut_point]\n",
    "                    \n",
    "            #loop through pgraph[x] using multiples of cutpoint to slice\n",
    "            #append subdoc at each slice\n",
    "            for i in range(0, (slices*2)):\n",
    "                pg = pgraphs[x][(cut_point*(i)):(cut_point*(i+1))]\n",
    "                sub_docs.append(pg)\n",
    "                \n",
    "        else:\n",
    "            sub_docs.append(pgraphs[x])\n",
    "        \n",
    "\n",
    "    \n",
    "    return sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing get_subdocs\n",
    "subs = get_subdocs(pgs, max_tokens)\n",
    "len(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running through sub_docs and outputting sentiment list\n",
    "\n",
    "def get_sentiments(sub_docs):\n",
    "    sentiments = []\n",
    "    for x in range(0,len(sub_docs)):\n",
    "        #print(len(tokenizer(sub_docs[x])['input_ids']))\n",
    "        s = classifier(sub_docs[x])\n",
    "        scr = s[0]['score']\n",
    "        if s[0]['label']==\"NEGATIVE\":\n",
    "            scr=scr*-1\n",
    "        sentiments.append(scr)\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8531283736228943, -0.9415674209594727, -0.9753469228744507, 0.631061851978302]\n"
     ]
    }
   ],
   "source": [
    "sents = get_sentiments(subs)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
